<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-09-09">
<meta name="description" content="The art of Reinforcement Learning in LLMs - PPO, GRPO and DPO.">

<title>Post training LLMs: The art of Reinforcement Learning ‚Äì Marcel Castro</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-ee47912cae73dc7932e3cc93c52697b6.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-RK1QNXR9TB"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-RK1QNXR9TB', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Marcel Castro</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://marcelcastrobr.github.io/my_ml_notes/"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Notebooks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://marcelcastrobr.github.io/my_ml_links/"> 
<span class="menu-text">Links</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/marcelcastrobr/marcelcastrobr.github.io/blob/main/cv/cv-marcelcastro.pdf"> <i class="bi bi-filetype-pdf" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://marcelcastrobr.github.io/my_ml_links/"> <i class="bi bi-diagram-3-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://huggingface.co/marcelcastrobr"> <i class="bi bi-emoji-smile" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/marcelcastrobr"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/marcelcastrobr/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Post training LLMs: The art of Reinforcement Learning</h1>
                  <div>
        <div class="description">
          The art of Reinforcement Learning in LLMs - PPO, GRPO and DPO.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">LLM, RL, Reasoning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 9, 2025</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">February 2, 2026</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#post-training-llms-the-art-of-reinforcement-learning" id="toc-post-training-llms-the-art-of-reinforcement-learning" class="nav-link active" data-scroll-target="#post-training-llms-the-art-of-reinforcement-learning">‚ú® Post training LLMs: The art of Reinforcement Learning‚Äù ‚ú®</a>
  <ul class="collapse">
  <li><a href="#rl-optimizers" id="toc-rl-optimizers" class="nav-link" data-scroll-target="#rl-optimizers">RL Optimizers</a></li>
  <li><a href="#ppo-proximal-policy-optimization" id="toc-ppo-proximal-policy-optimization" class="nav-link" data-scroll-target="#ppo-proximal-policy-optimization"><strong>PPO (Proximal Policy Optimization)</strong></a></li>
  <li><a href="#dpo-direct-preference-optimization" id="toc-dpo-direct-preference-optimization" class="nav-link" data-scroll-target="#dpo-direct-preference-optimization">DPO( Direct Preference Optimization)</a></li>
  <li><a href="#grpo-group-relative-policy-optimization." id="toc-grpo-group-relative-policy-optimization." class="nav-link" data-scroll-target="#grpo-group-relative-policy-optimization.">GRPO (Group Relative Policy Optimization).</a></li>
  <li><a href="#the-aha-moment" id="toc-the-aha-moment" class="nav-link" data-scroll-target="#the-aha-moment">üí° <strong>The ‚Äúaha moment‚Äù</strong> :</a></li>
  <li><a href="#rejection-sampling-and-sft" id="toc-rejection-sampling-and-sft" class="nav-link" data-scroll-target="#rejection-sampling-and-sft">üìù <strong>Rejection sampling and SFT</strong></a></li>
  <li><a href="#rl-for-all-scenarios" id="toc-rl-for-all-scenarios" class="nav-link" data-scroll-target="#rl-for-all-scenarios">üåê <strong>RL for all scenarios</strong></a></li>
  <li><a href="#distilled-models" id="toc-distilled-models" class="nav-link" data-scroll-target="#distilled-models">üß™ <strong>Distilled Models: </strong></a></li>
  <li><a href="#other-interesting-points" id="toc-other-interesting-points" class="nav-link" data-scroll-target="#other-interesting-points">üîç <strong>Other interesting points:</strong></a></li>
  <li><a href="#conclusion-the-future-implications-of-deepseek-r1-in-ai" id="toc-conclusion-the-future-implications-of-deepseek-r1-in-ai" class="nav-link" data-scroll-target="#conclusion-the-future-implications-of-deepseek-r1-in-ai">üöÄ Conclusion: The Future Implications of DeepSeek-R1 in AI</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">üìö<strong>References:</strong></a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="post-training-llms-the-art-of-reinforcement-learning" class="level1">
<h1>‚ú® Post training LLMs: The art of Reinforcement Learning‚Äù ‚ú®</h1>
<p>Current state of the art LLMs are trained in different stages, which are:</p>
<ul>
<li><strong>Pre-training</strong>: process where the LLM scratches over a large amount of data using next-token prediction.</li>
<li><strong>Supervised FineTunning (SFT)</strong>: where next-token prediction training is done through a supervised set of high-quality completions.</li>
<li><strong>Reinforcement Learning from Human Feedback (RLHF)</strong>: where Reinforcement Learning (RL) is used to train LLMs over human preference data.</li>
<li><strong>Reinforcement Learning from Verifiable Rewards (RLVR)</strong>: where RL is used on verifiable tasks (i.e.&nbsp;tasks where the ground throught is know a priori) and rewards can be derived from rules or heuristics.</li>
</ul>
<p><strong>RLHF</strong> and <strong>RLVR</strong> are alignment techniques in order to capture human preferences. For example in RLHF, the goal is to learn a policy by maximizing some reward from the environment. In RLHF the reward model is a proxy of the environment - i.e.&nbsp;LLM generates a completion over a set of prompts and compute the reward of these completions and use the rewards to derive a <strong>policy update</strong> (i.e.&nbsp;pdate of LLM¬¥s parameters) with an RL optimizer.</p>
<section id="rl-optimizers" class="level3">
<h3 class="anchored" data-anchor-id="rl-optimizers">RL Optimizers</h3>
<p>The RL optimization function is a maximization of the reward while minimizing the <strong>KL Divergence</strong> of the model with respect to a reference model (initial checkpoint of the model). KL Divergence or <strong>Kullback-Leibler divergence</strong> (introduced in my other blog here XXX) is a statistical measure used in machine learning to quantify the difference between two probability distribution.</p>
<p><strong>PPO</strong> and <strong>GPRO</strong> are example of on-line policy updates techniques. While <strong>DPO</strong> is an example of off-line technique (a.k.a. direct alignment ).</p>
</section>
<section id="ppo-proximal-policy-optimization" class="level3">
<h3 class="anchored" data-anchor-id="ppo-proximal-policy-optimization"><strong>PPO (Proximal Policy Optimization)</strong></h3>
<p>Initially RLHF models were finetunned using PPO .</p>
</section>
<section id="dpo-direct-preference-optimization" class="level3">
<h3 class="anchored" data-anchor-id="dpo-direct-preference-optimization">DPO( Direct Preference Optimization)</h3>
<p>DPO become popular due to its offline training capabilities. It can be used with verified rewards or rewards models</p>
</section>
<section id="grpo-group-relative-policy-optimization." class="level3">
<h3 class="anchored" data-anchor-id="grpo-group-relative-policy-optimization.">GRPO (<a href="https://arxiv.org/abs/2402.03300">Group Relative Policy Optimization</a>).</h3>
<p>GRPO (<a href="https://arxiv.org/abs/2402.03300">Group Relative Policy Optimization</a>), unlike traditional RL methods does not rely on external evaluators (critics) to guide learning. GRPO optimizes the model by evaluating groups of responses relative to one another.</p>
<p>GRPO mainly uses 2 reward functions:</p>
<ul>
<li><p>accuracy rewards: which evaluates whether the response is correct. (e.g.&nbsp;math problem with deterministic results and final answer, unit tests for code as accuracy computation).</p></li>
<li><p>format rewards: which enforces thinking process by rewarding model if it separates the ‚Äúthinking‚Äù and the ‚Äúanswer‚Äù parts by <think> tags.</think></p></li>
</ul>
<p>The figure below by <a href="https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1">Jay Alammar</a>, provides an excellent illustration example of such RL technique using reward signals. More information on GRPO on the paper <a href="https://arxiv.org/abs/2402.03300">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./assets/image-20250129083112616.png" class="img-fluid figure-img"></p>
<figcaption>Large-scale RL by Jay Alammar</figcaption>
</figure>
</div>
</section>
<section id="the-aha-moment" class="level3">
<h3 class="anchored" data-anchor-id="the-aha-moment">üí° <strong>The ‚Äúaha moment‚Äù</strong> :</h3>
<p>The model self-evolution through RL indicates its capability to reflect by revisiting and reevaluating previous steps and exploring alternative approaches through problem solving by using extended test-time computing during reasoning.</p>
<blockquote class="blockquote">
<p>‚ÄúThe self-evolution process of DeepSeek-R1 is fascinating as it demonstrate how RL can drive the model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, the author could monitor model progression without the influence of the supervised fine -tuning stage - indicating how model evolves overtime and its ability to handle complex reasoning tasks by leveraging extended <strong>test-time computing</strong>.‚Äù</p>
</blockquote>
<p>In addition to RL, DeepSeek-R1-Zero can be further augmented through the application of majority voting.</p>
</section>
<section id="rejection-sampling-and-sft" class="level3">
<h3 class="anchored" data-anchor-id="rejection-sampling-and-sft">üìù <strong>Rejection sampling and SFT</strong></h3>
<p>Rejection sampling and SFT is applied using data from other domains to enhance the model‚Äôs capability in writing, role-playing and other general-purpose tasks. For reasoning data this is done using generative reward model with ground-truth and DeepSeek-v3 as a judge (i.e.&nbsp;600k reasoning training samples in total). For non-reasoning data such as writing, factual QA, self-cognition and translation, the DeepSeek-v3 pipeline is used including reuse of portions of the SFT dataset of DeepSeek-v3 (200k training samples in total).</p>
</section>
<section id="rl-for-all-scenarios" class="level3">
<h3 class="anchored" data-anchor-id="rl-for-all-scenarios">üåê <strong>RL for all scenarios</strong></h3>
<p>RL is used to further align the model with human preferences through a secondary RL stage to improve model helpfulness and harmlessness. Rule-based rewards is used to guide the learn process in math, code and logical reasoning domains, using the same distribution of preference pairs and training prompts used by <a href="https://arxiv.org/pdf/2412.19437v1">DeepSeek-v3</a> pipeline.</p>
</section>
<section id="distilled-models" class="level3">
<h3 class="anchored" data-anchor-id="distilled-models">üß™ <strong>Distilled Models: </strong></h3>
<p>Fine-tuning is used as a distillation method to empower small models with reason capabilities like DeepSeek-R1. DeepSeek released 6 dense models (1.5B - 70B range) based on Qwen/Llama and distilled from DeepSeek-R1 using 800k curated samples. For distilled modes only SFT is applied (no RL stage included).</p>
</section>
<section id="other-interesting-points" class="level3">
<h3 class="anchored" data-anchor-id="other-interesting-points">üîç <strong>Other interesting points:</strong></h3>
<ul>
<li>Despite advocating that model distillation are both economical and effective methods, the DeepSeek‚Äôs authors highlight that advancements beyond the boundaries of intelligence may still require <strong>powerful base models</strong> and <strong>large-scale RL</strong>.</li>
<li>Monte Carlo Tree Search (MCTS), which is used by AlphaGo and AlphaZero, has also been proposed as a technique to enhance test-time compute scalability. But DeepSeek‚Äôs authors has seen scaling limitation during training as token generation presents an exponentially large search space compared to chess.</li>
<li><a href="https://arxiv.org/pdf/2412.19437v1">DeepSeek-v3-base</a> is used as the base model for DeepSeek-R1 and follows a <strong>Mixture of Expert (MoE)</strong> architecture. It has 671 billion parameters where 37 billion is activated for each token. See my previous post <a href="https://marcelcastrobr.github.io/posts/2024-05-19-UnderstandingMistureOfExperts.html">Understanding Mixture of Expert</a> for additional information on MoE architectures.</li>
<li>DeepSeek-v3-base uses <strong>Multi-Head Latent Attention (MLA)</strong> as its attention mechanism. MLA proposes a low-rank joint compression for the attention keys and values in order to reduce KV (Key-Value) cache during inference. See the Multi-Head Latent Attention section in my post <a href="https://marcelcastrobr.github.io/posts/2025-01-03-OptimizingLLMAttention.html#multi-head-latent-attention">The Power of Focus: Understanding Attention Mechanisms in LLM</a> for more information and references.</li>
</ul>
</section>
<section id="conclusion-the-future-implications-of-deepseek-r1-in-ai" class="level3">
<h3 class="anchored" data-anchor-id="conclusion-the-future-implications-of-deepseek-r1-in-ai">üöÄ Conclusion: The Future Implications of DeepSeek-R1 in AI</h3>
<p>DeepSeek-R1 shows how large-scale Reinforcement Learning (RL) can improve reasoning without extensive supervised fine-tuning (SFT). By using multi-stage training and cold-start data, DeepSeek-R1 matches the performance of top models.</p>
<p>Key points highlighted in this article include:</p>
<ul>
<li><strong>Multi-stage Training</strong>: Combining cold-start data, RL, and SFT to enhance the model.</li>
<li><strong>Reinforcement Learning</strong>: Using Group Relative Policy Optimization (GRPO) to improve responses.</li>
<li><strong>Reasoning Tokens</strong>: Generating ‚Äúthinking tokens‚Äù to boost reasoning.</li>
<li><strong>Self-Evolution</strong>: The model improves itself through RL.</li>
<li><strong>Distilled Models</strong>: Smaller models gain similar reasoning abilities through fine-tuning.</li>
</ul>
<p>DeepSeek-R1‚Äôs success highlights the potential of RL and reasoning tokens for creating smarter AI. This model opens new possibilities for AI applications in various fields, from language processing to decision-making.</p>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">üìö<strong>References:</strong></h3>
<ul>
<li><a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf">DeepSeek-R1 Technical Report</a></li>
<li><a href="https://arxiv.org/abs/2501.12948">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning by DeepSeek-AI</a></li>
<li><a href="https://medium.com/@sahin.samia/the-math-behind-deepseek-a-deep-dive-into-group-relative-policy-optimization-grpo-8a75007491ba">The Math Behind DeepSeek: A Deep Dive into Group Relative Policy Optimization (GRPO) by Ahmed</a></li>
<li><a href="https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1">The Ilustrated DeepSeek-R1 by Jay Alammar</a></li>
<li><a href="https://arxiv.org/abs/2403.09629">Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking</a></li>
</ul>
<blockquote class="blockquote">
<p>[!TIP]</p>
<p>For a fully open reproduction of DeepSeek-R1, check <a href="https://github.com/huggingface/open-r1">Open R1 project by Hugging Face</a>.</p>
<p>By the way you can use the <a href="https://apps.apple.com/in/app/deepseek-ai-assistant/id6737597349">DeepSeek‚Äôs AI assistant</a> app in the Apple App Store.</p>
</blockquote>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/marcelcastrobr\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "marcelcastrobr/marcelcastrobr.github.io";
    script.dataset.repoId = "R_kgDOIwFH3A";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOIwFH3M4Cjnq1";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2023, Marcel Castro</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>