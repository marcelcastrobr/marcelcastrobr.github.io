<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-05-25">
<meta name="description" content="Overview of current Large Language Model Optimization techniques for both inference and training">

<title>Overview of LLM Optimizations Techniques – Marcel Castro</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-2f9bc8108cc5788f896f6d8ac1f323a2.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-RK1QNXR9TB"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-RK1QNXR9TB', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Marcel Castro</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://marcelcastrobr.github.io/my_ml_notes/"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Notebooks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://marcelcastrobr.github.io/my_ml_links/"> 
<span class="menu-text">Links</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/marcelcastrobr/marcelcastrobr.github.io/blob/main/cv/cv-marcelcastro.pdf"> <i class="bi bi-filetype-pdf" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://marcelcastrobr.github.io/my_ml_links/"> <i class="bi bi-diagram-3-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://huggingface.co/marcelcastrobr"> <i class="bi bi-emoji-smile" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/marcelcastrobr"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/marcelcastrobr/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Overview of LLM Optimizations Techniques</h1>
                  <div>
        <div class="description">
          Overview of current Large Language Model Optimization techniques for both inference and training
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">machine_learning, transformers, LLM</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 25, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview-of-llm-optimizations-techniques" id="toc-overview-of-llm-optimizations-techniques" class="nav-link active" data-scroll-target="#overview-of-llm-optimizations-techniques">Overview of LLM Optimizations Techniques</a></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background">Background</a>
  <ul class="collapse">
  <li><a href="#flashattention" id="toc-flashattention" class="nav-link" data-scroll-target="#flashattention">FlashAttention</a>
  <ul class="collapse">
  <li><a href="#flashattention-solution" id="toc-flashattention-solution" class="nav-link" data-scroll-target="#flashattention-solution">FlashAttention Solution</a></li>
  <li><a href="#flashattention-at-pytorch" id="toc-flashattention-at-pytorch" class="nav-link" data-scroll-target="#flashattention-at-pytorch">FlashAttention at PyTorch</a></li>
  </ul></li>
  <li><a href="#pagedattention" id="toc-pagedattention" class="nav-link" data-scroll-target="#pagedattention">PagedAttention</a></li>
  <li><a href="#model-pruning" id="toc-model-pruning" class="nav-link" data-scroll-target="#model-pruning">Model Pruning</a></li>
  <li><a href="#knowledge-distillation" id="toc-knowledge-distillation" class="nav-link" data-scroll-target="#knowledge-distillation">Knowledge Distillation</a></li>
  <li><a href="#quantization" id="toc-quantization" class="nav-link" data-scroll-target="#quantization">Quantization</a>
  <ul class="collapse">
  <li><a href="#floating-point" id="toc-floating-point" class="nav-link" data-scroll-target="#floating-point">Floating Point</a></li>
  <li><a href="#precision-format" id="toc-precision-format" class="nav-link" data-scroll-target="#precision-format">Precision Format</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References:</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="overview-of-llm-optimizations-techniques" class="level1">
<h1>Overview of LLM Optimizations Techniques</h1>
<p>[TOC]</p>
</section>
<section id="background" class="level1">
<h1>Background</h1>
<p>Techniques such as quantization and distilation has been used to reduce model size.</p>
<p>For example the <strong><a href="https://arxiv.org/abs/2208.07339">Int8</a></strong> inference can reduce memory footprint of large models by a factor of 2x.</p>
<p>The sequencial generation process of autoregressive transformer which generates words (tokens) one at a time based on the input prompt and previous sequence, makes the GPU workload <strong>memory-bound</strong>, underutilizing the computation power of GPUs and limiting the serving throughput.</p>
<p>In order to make sure workloads are predominatly <strong>compute-bound</strong> at GPUs (and not memory-bound) several solutions have been proposed including <strong>FlashAttention</strong> and <strong>PagedAttention</strong> discussed below.</p>
<section id="flashattention" class="level2">
<h2 class="anchored" data-anchor-id="flashattention">FlashAttention</h2>
<p>The attention layer is the main bottleneck in scaling longer sequences, as its runtime and memory increase quadratically in the sequence length [ref. <a href="https://arxiv.org/abs/2307.08691">FlashAttention-2</a>].</p>
<p>To reduce computational requirement of attention on such long context, techniques such as <strong><em>FlashAttention</em></strong> has been proposed to reorder the attention computation and leverages classical techniques such as tilling and recomputation, to speed up and reduce memory usage from quadratic to linear in sequence length (<strong>2-4x faster</strong> than a standard attention implementation).</p>
<p>To check the list of flash attention adopters, check https://github.com/Dao-AILab/flash-attention/blob/main/usage.md</p>
<p>Before diving into Flash Attention we need to introduce the GPU hardware characteristics</p>
<section id="gpu-hardware-characteristics" class="level5">
<h5 class="anchored" data-anchor-id="gpu-hardware-characteristics">GPU Hardware Characteristics</h5>
<p>Main components of a modern GPU are:</p>
<ul>
<li>On-chip SRAM (a.k.a. as shared memory e.g.&nbsp;A100 19TB/s - 192KB per 108 streaming)</li>
<li>HBM (High Bandwidth Memory) (e.g.&nbsp;A100 - 40-80GB and 1.5-2.0TB/s )</li>
<li>SM (streaming multiprocessor) (e.g.&nbsp;A100 - 108 stream multiprocessors )
<ul>
<li>1 SM - 1 Thread block -&gt; Warp (1 warp - 32 threads)</li>
</ul></li>
</ul>
<p><img src="./images/image-20240506084744829.png" alt="image-20240506084744829" style="zoom:50%;"></p>
<p>Picture By DeepLearning Hero. How does matrix multiplication work inside GPUs - https://www.youtube.com/watch?v=wIPdrbZIeKE</p>
<p>Operations are executed in threads (a.k.a. kernel). Threads are organized into thread blocks, which are scheduled to run on streaming mutiprocessors (SMs)</p>
<p>Within each thread blocks, threads are grouped into warps ( 1 warp equal 3 threads). Threads within a warp can communicate by fast shuffe instructions or cooperate to perform matrix multiply.</p>
<p>Warps within a thread block can communicate by reading from and writing to shared memory. Each kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/image-20240506135327900.png" class="img-fluid figure-img"></p>
<figcaption>image-20240506135327900</figcaption>
</figure>
</div>
<p>Picture By DeepLearning Hero. How does matrix multiplication work inside GPUs - https://www.youtube.com/watch?v=wIPdrbZIeKE</p>
<p><img src="./images/image-20240506094321755.png" alt="image-20240506094321755" style="zoom:50%;"></p>
<p>Picture by Dissecting the Ampere GPU Architecture through Microbenchmarking - https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s33322/</p>
<p>Table below shows the comparison between certain GPU types across its main characteristics.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>GPU Type</th>
<th>Architecture</th>
<th>SM</th>
<th>HBM</th>
<th>SRAM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://www.nvidia.com/en-us/data-center/a100/">A100</a></td>
<td>NVIDIA Ampere</td>
<td>108</td>
<td>40 GB (1.5TB/s)</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">H100</a></td>
<td>NVIDIA Hopper</td>
<td>144</td>
<td>80GB(2.0TB/s)</td>
<td></td>
<td>Tensor Memory Accelerator, DPX (Dinamic Programmingh) Instructions, FP8 Tensor Cores, DSMEM (Distributed Shared Memory)</td>
</tr>
<tr class="odd">
<td><a href="https://www.nvidia.com/en-us/data-center/h200/">H200</a></td>
<td>NVIDIA Hopper</td>
<td></td>
<td>141GB (4.8TB/s)</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="flashattention-solution" class="level3">
<h3 class="anchored" data-anchor-id="flashattention-solution">FlashAttention Solution</h3>
<p>Lots of data loading turns computation in memory bound and not compute bound.</p>
<p>For each attention head, to reduce memory reads/writes, <strong>FlashAttention</strong> uses classical tiling techniques to load blocks of query, key, and value from GPU HBM (its main memory) to SRAM (its fast cache), compute attention with respect to that block, and write back the output to HBM. This reduction in memory reads/writes brings significant speedup (2-4x) in most cases. [ref. https://www.adept.ai/blog/flashier-attention]</p>
<p>The figure below is from <a href="https://arxiv.org/pdf/2205.14135">FlashAttention paper</a> showing on the left that FlashAttention uses tiling to prevent materialization of the large 𝑁 × 𝑁 attention matrix (dotted box) on (relatively) slow GPU HBM.</p>
<p>Here in the outer loop (red arrows), FlashAttention loops through blocks of the K and V matrices and loads them to fast on-chip SRAM. In each block, FlashAttention loops over blocks of Q matrix (blue arrows), loading them to SRAM, and writing the output of the attention computation back to HBM.</p>
<p>On the right you see the speedup over the PyTorch implementation of attention on GPT-2. FlashAttention does not read and write the large 𝑁 × 𝑁 attention matrix to HBM, resulting in an 7.6× speedup on the attention computation according to the paper.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/image-20240507155528488.png" class="img-fluid figure-img"></p>
<figcaption>image-20240507155528488</figcaption>
</figure>
</div>
</section>
<section id="flashattention-at-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="flashattention-at-pytorch">FlashAttention at PyTorch</h3>
<p>PyTorch implements flash attention (https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html).</p>
<p>Ref. code snipped example below from <a href="https://github.com/thushv89/tutorials_deeplearninghero/blob/master/llms/flash_attention_torch.ipynb">DeepLearning Hero</a>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.backends.cuda.sdp_kernel(</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>        enable_flash<span class="op">=</span><span class="va">False</span>, enable_math<span class="op">=</span><span class="va">True</span>, enable_mem_efficient<span class="op">=</span><span class="va">True</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>):</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Check https://marcelcastrobr.github.io/ for a notebook implementation.</p>
<p><strong>References:</strong></p>
<ul>
<li>[Ref 0] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness - https://arxiv.org/abs/2205.14135</li>
<li>[Ref1] FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning https://arxiv.org/abs/2307.08691</li>
<li>[Ref2] Matrix Multiplication: Inner Product, Outer Product &amp; Systolic Array https://www.adityaagrawal.net/blog/architecture/matrix_multiplication</li>
<li>[ref3] Benchmarking and Dissecting the Nvidia Hopper GPU Architecture https://arxiv.org/pdf/2402.13499v1</li>
<li>[ref4] NVIDIA Hopper Architecture In-Depth https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/</li>
<li>[ref5] FlashAttention: Fast Transformer training with long sequences https://www.adept.ai/blog/flashier-attention</li>
<li>[ref6] AWS Broadcast https://broadcast.amazon.com/videos/670513?ref=personal</li>
<li>[ref.7] Andrej Karpathy implementation - https://twitter.com/karpathy/status/1786461447654125625?s=51</li>
</ul>
</section>
</section>
<section id="pagedattention" class="level2">
<h2 class="anchored" data-anchor-id="pagedattention">PagedAttention</h2>
<p><strong>Issue</strong>: key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size.</p>
<p>PageAttention propose to improve the LLM throughput by batching multiple requests together. Thus to process many requests in a batch needs an efficient memory management.</p>
<p>As discussed in [Ref 0] , approximately 65% of memory is allocated to mode weights which are static parameters during serving (considing a 13B LLM on NVIDIA A100 GPU with 40GB RAM). Close to 30% of the memory is used to store the dynamic states of the requests (a.k.a <strong>KV cache</strong>). The remaining memore is used for other data such as activations.</p>
<p>Since model weights are constant and the activations only occupy a small fraction of the GPU memory, the way KV cache if managed is critical in determinig the maximum batch size.</p>
<p><img src="./images/image-20240513100159526.png" alt="image-20240513100159526" style="zoom:50%;"></p>
<p>Picture from Paper Efficient Memory Management for Large Language Model Serving with PagedAttention. In the Left: Memory layout when serving an LLM with 13B parameters on NVIDIA A100. The parameters (gray) persist in GPU memory throughout serving. The memory for the KV cache (red) is (de)allocated per serving request. A small amount of memory (yellow) is used ephemerally for activation. Right: vLLM smooths out the rapid growth curve of KV cache memory seen in existing systems [31, 60], leading to a notable boost in serving throughput.</p>
<p>PageAttention solution is inspired by the operating system solution to memory fragmentation and sharing: <em>virtual memory with paging</em>. KV cache is divided into blocks, where the blocks are not necessarly stored in contiguous space (i.e.&nbsp;think blocks as pages, tokens as bytes and requests as processes). This allows PagedAttention to achieve:</p>
<ul>
<li>near-zero waste in KV cache memory.</li>
<li>flexible sharing of KV cache within and across requests to further reduce memory usage.</li>
</ul>
<p><strong>References:</strong></p>
<p>[Ref 0] [Efficient Memory Management for Large Language Model Serving with PagedAttention - https://arxiv.org/pdf/2309.06180]</p>
</section>
<section id="model-pruning" class="level2">
<h2 class="anchored" data-anchor-id="model-pruning">Model Pruning</h2>
<p>Ref [] Techniques for Efficient Inference of LLMs (II/IV) - https://medium.com/mantisnlp/techniques-for-efficient-inference-of-llms-ii-iv-5324f3dad69c</p>
</section>
<section id="knowledge-distillation" class="level2">
<h2 class="anchored" data-anchor-id="knowledge-distillation">Knowledge Distillation</h2>
</section>
<section id="quantization" class="level2">
<h2 class="anchored" data-anchor-id="quantization">Quantization</h2>
<p>Backgroun problem: Larger models but reduce memory capacity on accelerators.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/image-20240422115546209.png" class="img-fluid figure-img"></p>
<figcaption>image-20240422115546209</figcaption>
</figure>
</div>
<p>Source: Deeplearning.ai course <a href="https://learn.deeplearning.ai/courses/quantization-fundamentals/lesson/2/handling-big-models">here</a></p>
<p>Key facts:</p>
<ul>
<li>you can quantize the model weights and activations</li>
</ul>
<p>Example:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/image-20240422120051189.png" class="img-fluid figure-img"></p>
<figcaption>image-20240422120051189</figcaption>
</figure>
</div>
<section id="floating-point" class="level3">
<h3 class="anchored" data-anchor-id="floating-point">Floating Point</h3>
<p>Floating point is defined by three components:</p>
<ul>
<li><p>Sign: positive/negative (1 bit)</p></li>
<li><p>Exponent (range): impact the representable range of the number</p></li>
<li><p>Fraction (precision): impact on the precision of the number</p></li>
</ul>
<p><strong>Downcasting:</strong> Loss of data due to the convertion of higher data type (e.g.&nbsp;float) to a lower data type (integer)</p>
<p>Advantages of Downcasting is:</p>
<ul>
<li>reduce memory footprint: more efficient use of memory, enable training of larger models and largers batch sizes</li>
<li>increase compute and speed: low precision (fp16, bf16) can be faster than fp32 since it uses less memory.</li>
</ul>
<p>But disadvantages comes to the less precise computation.</p>
<p>Usecase for Downcasting are:</p>
<ul>
<li>do computation in smaller precison and store and update the weights in higher precision.</li>
</ul>
</section>
<section id="precision-format" class="level3">
<h3 class="anchored" data-anchor-id="precision-format">Precision Format</h3>
<p>Size of the model is determined by the number of its parameters and their precision.</p>
<p>Based on IEEE standard for floating point arithmetics, it is common convention to represent numbers in binary using 64bits for <strong>double-precision</strong>, 32 bits for <strong>single-precision</strong> and 16 bits for <strong>half-precision</strong>. In ML jargon FP32 is called full-precision (4 bytes) and FP16 are refered as half-precison (2 bytes).</p>
<p>For example, the float16 (FP16) data type, 5 bits are reserved for <strong>exponent</strong> (range) and 10 bits are reserved for <strong><a href="https://en.wikibooks.org/wiki/A-level_Computing/AQA/Paper_2/Fundamentals_of_data_representation/Floating_point_numbers">mantissa</a></strong> (precision/fraction) and 1 bit for sign of the number. Compared to FP32, the FP36 has a much lower range exposing it to the risk of <strong>overflowing</strong> (i.e.&nbsp;trying to represent a number that is very large) and <strong>underflowing</strong> (i.e.&nbsp;trying to represent a number that is very small).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/image-20230509074147204.png" class="img-fluid figure-img"></p>
<figcaption>image-20230509074147204</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/image-20240422121320403.png" class="img-fluid figure-img"></p>
<figcaption>image-20240422121320403</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/image-20240422121351859.png" class="img-fluid figure-img"></p>
<figcaption>image-20240422121351859</figcaption>
</figure>
</div>
<p>Example below from Coursera course: Generative AI with LLMs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/image-20230702151132607.png" class="img-fluid figure-img"></p>
<figcaption>image-20230702151132607</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/image-20230702151239388.png" class="img-fluid figure-img"></p>
<figcaption>image-20230702151239388</figcaption>
</figure>
</div>
<p>Nice to know: <a href="https://blogs.nvidia.com/blog/2019/11/15/whats-the-difference-between-single-double-multi-and-mixed-precision-computing/">Using the pi constant 3.14 E0</a>, the 3.14 is the precision and E0 the range Thus for FP16 we can have range of 2** 5 and precision of 2 **10.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/image-20230509081209750.png" class="img-fluid figure-img"></p>
<figcaption>image-20230509081209750</figcaption>
</figure>
</div>
<section id="full-and-half-precision-in-ml" class="level4">
<h4 class="anchored" data-anchor-id="full-and-half-precision-in-ml">Full and Half Precision in ML</h4>
<p>Since FP32 is 2 times slower than FP16, a mixed precision approach is used in ML, where the weights are held in FP32 as a precise main weights reference and forward and backward pass are done for FP16/BF16 to enhance training speed. Thus the FP16 gradients are used to update the FP32 main weights.</p>
<p><strong>Calculating Model Size</strong></p>
<p>To calculate model size in bytes, you need to multiply the number of parameters by the size of the chosen precision in bytes. <strong>E.g. using BF16 version of BLOOM-176B model we will need 176<em>E9 </em> 2 bytes = 352GBytes.</strong></p>
<p>Thus we need a few GPUs to do inference using Bloom-176B. But, luckily we can store the weights with less memory using different data types –&gt; technique called <strong>quantization</strong>. Two common 8-bit quantizations techniques are <strong>zero-pointy quantization</strong> and <strong>absolute maximum</strong> (absmax) quantization.</p>
</section>
<section id="model-quantization" class="level4">
<h4 class="anchored" data-anchor-id="model-quantization">Model Quantization</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/image-20240422123407667.png" class="img-fluid figure-img"></p>
<figcaption>image-20240422123407667</figcaption>
</figure>
</div>
<p>Example from fp32 to bf16:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/image-20240422124540042.png" class="img-fluid figure-img"></p>
<figcaption>image-20240422124540042</figcaption>
</figure>
</div>
<p>8-bit quantization method used a quarter precision, then reducing the model to 1/4th of its original size. Quantization is done by “rounding” from one data type to another. However this might lead to information loss (i.e.&nbsp;lossy compression)</p>
<p>LLM.int8() is an example of quantization implemented by HuggingFace Transformers. To achieve zero degradation matrix multiplication for LLM, LLM.int8() remove the performance deterioration caused by outlier features by identifying the outliers from the input hidden states and multiplying it in FP16 and non-outliers in int8.</p>
<p>A side effect of this quantization is the the model can suffer performance degradtion (15% to 23% slower than the FP16)</p>
</section>
<section id="memory-usage" class="level4">
<h4 class="anchored" data-anchor-id="memory-usage">Memory Usage</h4>
<p>Falcon Models</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Layers</th>
<th>d_model</th>
<th>Head_dim</th>
<th>Vocabulary</th>
<th>Sequence length</th>
<th>Hardware</th>
<th>GPU Memory required</th>
<th>Pretraining length [tokens]</th>
<th>Pretraining compute [PF-days]</th>
<th>K,V-cache size for a 2.048 context</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>tiiuae/falcon-40b-instruct</td>
<td>60</td>
<td>8192</td>
<td>64</td>
<td>65024</td>
<td>2048</td>
<td>64 A100 40GB in p4d.</td>
<td><a href="https://huggingface.co/blog/falcon">~90GB</a></td>
<td>1.0 Trillion</td>
<td>2800</td>
<td>20MB</td>
</tr>
<tr class="even">
<td>tiiuae/falcon-7b-instruct</td>
<td>32</td>
<td>4544</td>
<td>64</td>
<td>65024</td>
<td>2048</td>
<td>32 A100 40GB GPUs in P4d.</td>
<td><a href="https://huggingface.co/blog/falcon">~15GB</a></td>
<td>1.5 Trillion</td>
<td>700</td>
<td>240MB</td>
</tr>
</tbody>
</table>
<p>AWS Instances:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 7%">
<col style="width: 8%">
<col style="width: 5%">
<col style="width: 12%">
<col style="width: 13%">
<col style="width: 18%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th>Instance</th>
<th>Memory</th>
<th>vCPUs</th>
<th>GPUs</th>
<th>GPU Memory</th>
<th>GPU Model</th>
<th>Storage</th>
<th>Price</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>g5.12xlarge</td>
<td>192GB</td>
<td>48vCPU</td>
<td>4</td>
<td>96GiB</td>
<td>NVIDIA A10G</td>
<td>3800GB NVMe SSD</td>
<td>$6.3317 hourly</td>
</tr>
<tr class="even">
<td>g5.48xlarge</td>
<td>768GB</td>
<td>192vCPU</td>
<td>8</td>
<td>192GiB</td>
<td>NVIDIA A10G</td>
<td>7600GB</td>
<td>$18.1823 hourly</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Reference:</strong></p>
<ul>
<li><a href="https://huggingface.co/docs/transformers/quantization?bnb=4-bit">Quantization by HuggingFace</a></li>
</ul>
</section>
</section>
</section>
</section>
<section id="references" class="level1">
<h1>References:</h1>
<ul>
<li><a href="https://www.youtube.com/watch?v=QQceTDjA4f4">GTC 2022 - How CUDA Programming Works - Stephen Jones, CUDA Architect, NVIDIA.</a></li>
<li><a href="https://huggingface.co/blog/hf-bitsandbytes-integration">A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes</a></li>
<li><a href="https://lightning.ai/pages/community/community-discussions/the-ultimate-battle-of-language-models-lit-llama-vs-gpt3.5-vs-bloom-vs/">The Ultimate Battle of Language Models: Lit-LLaMA vs GPT3.5 vs Bloom vs …</a></li>
<li><a href="https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.jumpstart.model.JumpStartModel">SageMaker JumpStartModel API Docs</a></li>
<li>Video: <a href="https://www.youtube.com/watch?v=ahnGLM-RC1Y">A Survey of Techniques for Maximizing LLM Performance, by OpenAI DevDay</a></li>
<li>Yann Lecun, New York University &amp; META Title: Objective-Driven AI: Towards AI systems that can learn, remember, reason, and plan, <a href="https://www.youtube.com/watch?v=MiqLoAZFRSE&amp;t=3983s">video</a>, <a href="https://drive.google.com/file/d/1wzHohvoSgKGZvzOWqZybjm4M4veKR6t3/view?pli=1">ppt</a></li>
<li>IEEE Spectrum - 15 Graphs That Explain the State of AI in 2024 The AI Index tracks the generative AI boom, model costs, and responsible AI use https://spectrum.ieee.org/ai-index-2024</li>
<li><a href="https://modal.com/gpu-glossary/readme">GPU Glossary by Modal</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/marcelcastrobr\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "marcelcastrobr/marcelcastrobr.github.io";
    script.dataset.repoId = "R_kgDOIwFH3A";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOIwFH3M4Cjnq1";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2023, Marcel Castro</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>