{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /kerastuner/tensorflow/2021/09/05/KerasTuner\n",
    "badges: true\n",
    "categories:\n",
    "- kerastuner\n",
    "- tensorflow\n",
    "date: '2021-09-05'\n",
    "description: Adaptation from notebook from C3_W1_Lab_1_Keras_Tuner from DeepLearningAi\n",
    "  MLOPs Specialization - Course 3\n",
    "output-file: 2021-09-05-kerastuner.html\n",
    "title: Using Keras Tuner for hyperparameter tunning\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReV_UXOgCZvx"
   },
   "source": [
    "## Download and prepare the dataset\n",
    "\n",
    "**Kaggle competition:** Tabular Playground Series - Aug 2021 -https://www.kaggle.com/c/tabular-playground-series-aug-2021\n",
    "\n",
    "**Dataset:** The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with calculating the loss associated with a loan defaults. Although the features are anonymized, they have properties relating to real-world features.\n",
    "\n",
    "**Target:** loss column\n",
    "\n",
    "**Notebook** The code of this notebook is inspired by the lab Intro to Keras Tuner from Robert Crowe used on Course Machine Learning Modeling Pipelines in Production by DeepLearning.AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysAmHLZoDld7"
   },
   "outputs": [],
   "source": [
    "# Import the packages\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# For visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For modelling\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column as fc\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set TF logger to only print errors (dismiss warnings)\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Euly5kZ8pa2M"
   },
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1CH9fc6bFzS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "for dirname, _, filenames in os.walk('./input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LLAa1jNph9l"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"/tmp/data\"):\n",
    "    os.makedirs(\"/tmp/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmbVoA2NpjaN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtJ5qLKHbK3O"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "#X = pd.read_csv('../input/tabular-playground-series-aug-2021/train.csv', index_col='id')\n",
    "#X_test_full = pd.read_csv('../input/tabular-playground-series-aug-2021/test.csv', index_col='id')\n",
    "\n",
    "Xx = pd.read_csv('./input/train.csv', index_col='id')\n",
    "#X_test = pd.read_csv('./input/test.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNRx_o2lbXlk"
   },
   "outputs": [],
   "source": [
    "# Remove rows with missing target, separate target from predictors\n",
    "X = Xx.copy(deep=True)\n",
    "X.dropna(axis=0, subset=['loss'], inplace=True)\n",
    "#Y = X.loss              \n",
    "#X.drop(['loss'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgorYxxybcR8"
   },
   "outputs": [],
   "source": [
    "# Describe columns\n",
    "X.describe(include='all').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boh55HvFf7zB"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "#X_test_index = X_test.index\n",
    "#X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_3J1MMJwcbT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eTtrgjKfYv6"
   },
   "outputs": [],
   "source": [
    "#X_train, X_valid, Y_train, Y_valid = train_test_split(X_scaled, Y, train_size=0.8, test_size = 0.2,stratify=Y, random_state = 123)\n",
    "train, valid = train_test_split(X, test_size=0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfeRFoiqeRX8"
   },
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOAFRcO-r073"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(train).to_csv(\"/tmp/data/tabular-train.csv\", index=False)\n",
    "pd.DataFrame(valid).to_csv(\"/tmp/data/tabular-valid.csv\", index=False)\n",
    "#pd.DataFrame(X_test_scaled).to_csv(\"/tmp/data/tabular-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yr3zgA4wsRha"
   },
   "outputs": [],
   "source": [
    "!ls -l /tmp/data/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuRfw9FMvRBe"
   },
   "outputs": [],
   "source": [
    "pd.read_csv('/tmp/data/tabular-train.csv').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O68S_PQcxN1T"
   },
   "outputs": [],
   "source": [
    "pd.read_csv('/tmp/data/tabular-valid.csv').pop('loss').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4v2ran10saD6"
   },
   "source": [
    "# Create input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YuN8gYGZseXX"
   },
   "outputs": [],
   "source": [
    "# Specify which column is the target\n",
    "LABEL_COLUMN = 'loss'\n",
    "\n",
    "# Specify numerical columns\n",
    "# Note you should create another list with STRING_COLS if you \n",
    "# had text data but in this case all features are numerical\n",
    "NUMERIC_COLS = ['f0', 'f1',\n",
    "                'f2', 'f3',\n",
    "                'f4', 'f5', 'f6']\n",
    "\n",
    "\n",
    "# A function to separate features and labels\n",
    "def features_and_labels(row_data):\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    return row_data, label\n",
    "\n",
    "\n",
    "# A utility method to create a tf.data dataset from a CSV file\n",
    "def load_dataset(pattern, batch_size=1, mode='eval'):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(pattern, batch_size)\n",
    "    \n",
    "    dataset = dataset.map(features_and_labels)  # features, label\n",
    "    if mode == 'train':\n",
    "        # Notice the repeat method is used so this dataset will loop infinitely\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "        # take advantage of multi-threading; 1=AUTOTUNE\n",
    "        dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BHh_hPhbpMo"
   },
   "source": [
    "# Building DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVzvY2kXtgxH"
   },
   "outputs": [],
   "source": [
    "def build_dnn_model():\n",
    "    # input layer\n",
    "    inputs = {\n",
    "        colname: layers.Input(name=colname, shape=(), dtype='float32')\n",
    "        for colname in NUMERIC_COLS\n",
    "    }\n",
    "\n",
    "    # feature_columns\n",
    "    feature_columns = {\n",
    "        colname: fc.numeric_column(colname)\n",
    "        for colname in NUMERIC_COLS\n",
    "    }\n",
    "\n",
    "    # Constructor for DenseFeatures takes a list of numeric columns\n",
    "    # and the resulting tensor takes a dictionary of Input layers\n",
    "    dnn_inputs = layers.DenseFeatures(feature_columns.values())(inputs)\n",
    "\n",
    "    # two hidden layers of 32 and 8 units, respectively\n",
    "    h1 = layers.Dense(32, activation='relu', name='h1')(dnn_inputs)\n",
    "    h2 = layers.Dense(8, activation='relu', name='h2')(h1)\n",
    "\n",
    "    # final output is a linear activation because this is a regression problem\n",
    "    output = layers.Dense(1, activation='linear', name='fare')(h2)\n",
    "\n",
    "    # Create model with inputs and output\n",
    "    model = models.Model(inputs, output)\n",
    "\n",
    "    # compile model (Mean Squared Error is suitable for regression)\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='mse', \n",
    "                  metrics=[\n",
    "                      tf.keras.metrics.RootMeanSquaredError(name='rmse'), \n",
    "                      'mse'\n",
    "                  ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alg2zaRFtmgM"
   },
   "outputs": [],
   "source": [
    "# Save compiled model into a variable\n",
    "model = build_dnn_model()\n",
    "\n",
    "# Plot the layer architecture and relationship between input features\n",
    "#tf.keras.utils.plot_model(model, 'dnn_model.png', show_shapes=False, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDn3wBPXtrJw"
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lELyg8VStqjE"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "TRAIN_BATCH_SIZE = 32 \n",
    "NUM_TRAIN_EXAMPLES = len(pd.read_csv('/tmp/data/tabular-train.csv'))\n",
    "NUM_EVAL_EXAMPLES = len(pd.read_csv('/tmp/data/tabular-valid.csv'))\n",
    "\n",
    "print(f\"training split has {NUM_TRAIN_EXAMPLES} examples\\n\")\n",
    "print(f\"evaluation split has {NUM_EVAL_EXAMPLES} examples\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmjVmn6KtzUs"
   },
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "trainds = load_dataset('/tmp/data/tabular-train*', TRAIN_BATCH_SIZE, 'train')\n",
    "\n",
    "# Evaluation dataset\n",
    "evalds = load_dataset('/tmp/data/tabular-valid*', 1000, 'eval').take(NUM_EVAL_EXAMPLES//1000)\n",
    "\n",
    "# Needs to be specified since the dataset is infinite \n",
    "# This happens because the repeat method was used when creating the dataset\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // TRAIN_BATCH_SIZE\n",
    "\n",
    "# Train the model and save the history\n",
    "history = model.fit(trainds,\n",
    "                    validation_data=evalds,\n",
    "                    epochs=NUM_EPOCHS\n",
    "                    ,steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdYN2zfTyt6p"
   },
   "outputs": [],
   "source": [
    "print(type(trainds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RI8d0pdoboEj"
   },
   "outputs": [],
   "source": [
    "# Callback funtion\n",
    "DESIRED_MAE = 5\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('mean_absolute_error') < DESIRED_MAE):\n",
    "      print(\"\\nReached {}% MAE so cancelling training!\".format(DESIRED_MAE))\n",
    "      self.model.stop_training = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksxSrowDtl-G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXoRucWcgQQ7"
   },
   "outputs": [],
   "source": [
    "# Number of training epochs.\n",
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzdrvBQKc6Il"
   },
   "outputs": [],
   "source": [
    "#inputs = keras.Input(shape=())\n",
    "inputs = keras.Input(shape=(X_train.shape[1]))\n",
    "\n",
    "model_dnn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(512,activation='relu',name='dense_1'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10,activation='relu',name='dense_2'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1,activation='relu',name='dense_3')\n",
    "])\n",
    "\n",
    "model_dnn.compile(loss=\"mean_absolute_error\", \n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "                  metrics=[\"mean_absolute_error\"])\n",
    "#model_dnn.summary()\n",
    "\n",
    "history = model_dnn.fit(X_train, Y_train,epochs=NUM_EPOCHS,\n",
    "                       callbacks=[myCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrZusLjbet8E"
   },
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "model_dnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQXIEY-sgPpv"
   },
   "outputs": [],
   "source": [
    "b_eval_dict = model_dnn.evaluate(X_valid, Y_valid, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YCfzg0IM9b6"
   },
   "source": [
    "Let's define a helper function for displaying the results so it's easier to compare later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vt2dWs0NxnUn"
   },
   "outputs": [],
   "source": [
    "# Define helper function\n",
    "def print_results(model, model_name, eval_dict):\n",
    "  '''\n",
    "  Prints the values of the hyparameters to tune, and the results of model evaluation\n",
    "\n",
    "  Args:\n",
    "    model (Model) - Keras model to evaluate\n",
    "    model_name (string) - arbitrary string to be used in identifying the model\n",
    "    eval_dict (dict) -  results of model.evaluate\n",
    "  '''\n",
    "  print(f'\\n{model_name}:')\n",
    "\n",
    "  print(f'number of units in 1st Dense layer: {model.get_layer(\"dense_1\").units}')\n",
    "  print(f'learning rate for the optimizer: {model.optimizer.lr.numpy()}')\n",
    "\n",
    "  for key,value in eval_dict.items():\n",
    "    print(f'{key}: {value}')\n",
    "\n",
    "# Print results for baseline model\n",
    "print_results(model_dnn, 'BASELINE MODEL', b_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AH-RLK3Wxt_X"
   },
   "source": [
    "That's it for getting the results for a single set of hyperparameters. \n",
    "Let´s use Keras Tuner by having an API to automatically search for the optimal hyperparameters set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oyczDXqtWjI"
   },
   "source": [
    "## Keras Tuner\n",
    "\n",
    "To perform hypertuning with Keras Tuner, you will need to:\n",
    "\n",
    "* Define the model\n",
    "* Select which hyperparameters to tune\n",
    "* Define its search space\n",
    "* Define the search strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUXex9ctTuDB"
   },
   "source": [
    "### Install and import packages\n",
    "\n",
    "You will start by installing and importing the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpMLpbt9jcO6"
   },
   "outputs": [],
   "source": [
    "# Install Keras Tuner\n",
    "!pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_leAIdFKAxAD"
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import tensorflow as tf\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5YEL2H2Ax3e"
   },
   "source": [
    "### Define the model\n",
    "\n",
    "The model you set up for hypertuning is called a *hypermodel*. When you build this model, you define the hyperparameter search space in addition to the model architecture. \n",
    "\n",
    "You can define a hypermodel through two approaches:\n",
    "\n",
    "* By using a model builder function\n",
    "* By [subclassing the `HyperModel` class](https://keras-team.github.io/keras-tuner/#you-can-use-a-hypermodel-subclass-instead-of-a-model-building-function) of the Keras Tuner API\n",
    "\n",
    "\n",
    "In this lab, you will take the first approach: you will use a model builder function to define the image classification model. This function returns a compiled model and uses hyperparameters you define inline to hypertune the model. \n",
    "\n",
    "The function below basically builds the same model you used earlier. The difference is there are two hyperparameters that are setup for tuning:\n",
    "\n",
    "* the number of hidden units of the first Dense layer\n",
    "* the learning rate of the Adam optimizer\n",
    "\n",
    "You will see that this is done with a HyperParameters object which configures the hyperparameter you'd like to tune. For this exercise, you will: \n",
    "\n",
    "* use its `Int()` method to define the search space for the Dense units. This allows you to set a minimum and maximum value, as well as the step size when incrementing between these values. \n",
    "\n",
    "* use its `Choice()` method for the learning rate. This allows you to define discrete values to include in the search space when hypertuning.\n",
    "\n",
    "You can view all available methods and its sample usage in the [official documentation](https://keras-team.github.io/keras-tuner/documentation/hyperparameters/#hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQKodC-jtsva"
   },
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  '''\n",
    "  Builds the model and sets up the hyperparameters to tune.\n",
    "\n",
    "  Args:\n",
    "    hp - Keras tuner object\n",
    "\n",
    "  Returns:\n",
    "    model with hyperparameters to tune\n",
    "  '''\n",
    "\n",
    "  # Initialize the Sequential API and start stacking the layers\n",
    "  model = keras.Sequential()\n",
    "  #model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "  model.add(keras.layers.Dense(units=hp_units, activation='relu', name='dense_1'))\n",
    "\n",
    "  # Add next layers\n",
    "  model.add(keras.layers.Dropout(0.2))\n",
    "  model.add(tf.keras.layers.Dense(10,activation='relu',name='dense_2'))\n",
    "  model.add(keras.layers.Dropout(0.2))\n",
    "  model.add(tf.keras.layers.Dense(1,activation='relu',name='dense_3'))\n",
    "\n",
    "  # Tune the learning rate for the optimizer\n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=\"mean_absolute_error\",\n",
    "                metrics=['mean_absolute_error'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J1VYw4q3x0b"
   },
   "source": [
    "## Instantiate the Tuner and perform hypertuning\n",
    "\n",
    "Now that you have the model builder, you can then define how the tuner can find the optimal set of hyperparameters, also called the search strategy. Keras Tuner has [four tuners](https://keras-team.github.io/keras-tuner/documentation/tuners/) available with built-in strategies - `RandomSearch`, `Hyperband`, `BayesianOptimization`, and `Sklearn`. \n",
    "\n",
    "In this tutorial, you will use the Hyperband tuner. Hyperband is an algorithm specifically developed for hyperparameter optimization. It uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket wherein the algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. You can read about the intuition behind the algorithm in section 3 of [this paper](https://arxiv.org/pdf/1603.06560.pdf).\n",
    "\n",
    "Hyperband determines the number of models to train in a bracket by computing 1 + log<sub>`factor`</sub>(`max_epochs`) and rounding it up to the nearest integer. You will see these parameters (i.e. `factor` and `max_epochs` passed into the initializer below). In addition, you will also need to define the following to instantiate the Hyperband tuner:\n",
    "\n",
    "* the hypermodel (built by your model builder function)\n",
    "* the `objective` to optimize (e.g. validation accuracy)\n",
    "* a `directory` to save logs and checkpoints for every trial (model configuration) run during the hyperparameter search. If you re-run the hyperparameter search, the Keras Tuner uses the existing state from these logs to resume the search. To disable this behavior, pass an additional `overwrite=True` argument while instantiating the tuner.\n",
    "* the `project_name` to differentiate with other runs. This will be used as a subdirectory name under the `directory`.\n",
    "\n",
    "You can refer to the [documentation](https://keras.io/api/keras_tuner/tuners/hyperband/) for other arguments you can pass in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oichQFly6Y46"
   },
   "outputs": [],
   "source": [
    "# Instantiate the tuner\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_mean_absolute_error',\n",
    "                     max_epochs=5,\n",
    "                     factor=3,\n",
    "                     directory='kt_dir',\n",
    "                     project_name='kt_hyperband')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ij3hGcp4e8QG"
   },
   "source": [
    "Let's see a summary of the hyperparameters that you will tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmkJOPp5WkiG"
   },
   "outputs": [],
   "source": [
    "# Display hypertuning settings\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwhBdXx0Ekj8"
   },
   "source": [
    "You can pass in a callback to stop training early when a metric is not improving. Below, we define an [EarlyStopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) callback to monitor the validation loss and stop training if it's not improving after 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WT9IkS9NEjLc"
   },
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_mean_absolute_error', patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKghEo15Tduy"
   },
   "source": [
    "You will now run the hyperparameter search. The arguments for the search method are the same as those used for `tf.keras.model.fit` in addition to the callback above. This will take around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSBQcTHF9cKt"
   },
   "outputs": [],
   "source": [
    "# Perform hypertuning\n",
    "tuner.search(X_train, Y_train, epochs=NUM_EPOCHS, validation_split=0.2, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewN6WBDYWvRw"
   },
   "source": [
    "You can get the top performing model with the [get_best_hyperparameters()](https://keras-team.github.io/keras-tuner/documentation/tuners/#get_best_hyperparameters-method)  method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iG0zIuP5WuTI"
   },
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters from the results\n",
    "best_hps=tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lak_ylf88xBv"
   },
   "source": [
    "## Build and train the model\n",
    "\n",
    "Now that you have the best set of hyperparameters, you can rebuild the hypermodel with these values and retrain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "McO82AXOuxXh"
   },
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters\n",
    "h_model = tuner.hypermodel.build(best_hps)\n",
    "#h_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l64WP7Rau1lm"
   },
   "outputs": [],
   "source": [
    "# Train the hypertuned model\n",
    "h_model.fit(X_train, Y_train, epochs=NUM_EPOCHS, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqU5ZVAaag2v"
   },
   "source": [
    "You will then get its performance against the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9E0BTp9Ealjb"
   },
   "outputs": [],
   "source": [
    "# Evaluate the hypertuned model against the test set\n",
    "h_eval_dict = h_model.evaluate(X_valid, Y_valid, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQRpPHZsz-eC"
   },
   "source": [
    "We can compare the results we got with the baseline model we used at the start of the notebook. Results may vary but you will usually get a model that has less units in the dense layer, while having comparable loss and accuracy. This indicates that you reduced the model size and saved compute resources while still having more or less the same accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BjVYPOw6MH5d"
   },
   "outputs": [],
   "source": [
    "# Print results of the baseline and hypertuned model\n",
    "#print_results(b_model, 'BASELINE MODEL', b_eval_dict)\n",
    "print_results(h_model, 'HYPERTUNED MODEL', h_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKn4g_HzP2KS"
   },
   "source": [
    "## Possible Improvements\n",
    "\n",
    "If you want to keep practicing with Keras Tuner in this notebook, you can do a factory reset (`Runtime > Factory reset runtime`) and take on any of the following:\n",
    "\n",
    "- hypertune the dropout layer with `hp.Float()` or `hp.Choice()`\n",
    "- hypertune the activation function of the 1st dense layer with `hp.Choice()`\n",
    "- determine the optimal number of Dense layers you can add to improve the model. You can use the code [here](https://keras.io/guides/keras_tuner/getting_started/#the-search-space-may-contain-conditional-hyperparameters) as reference.\n",
    "- explore pre-defined `HyperModel` classes - [HyperXception and HyperResNet](https://keras-team.github.io/keras-tuner/documentation/hypermodels/#hyperresnet-class) for computer vision applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKwLOzKpFGAj"
   },
   "source": [
    "## Wrap Up\n",
    "\n",
    "In this tutorial, you used Keras Tuner to conveniently tune hyperparameters. You defined which ones to tune, the search space, and search strategy to arrive at the optimal set of hyperparameters. These concepts will again be discussed in the next sections but in the context of AutoML, a package that automates the entire machine learning pipeline. On to the next!\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C3_W1_Lab_1_Keras_Tuner.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
