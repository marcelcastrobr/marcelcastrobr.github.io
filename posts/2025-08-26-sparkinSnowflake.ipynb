{
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "lastEditStatus": {
   "notebookId": "hyurclg4fy7evsin4xcv",
   "authorId": "1758781709217",
   "authorName": "MCASTRO",
   "authorEmail": "marcel.castro@snowflake.com",
   "sessionId": "0fdb589e-fd7c-48d1-8402-ed2f5a5ef1af",
   "lastEditTime": 1756240709207
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14bfc485-8fd7-4d16-a802-8ebd82bb56bf",
   "metadata": {
    "name": "cell1",
    "collapsed": true
   },
   "source": [
    "---\n",
    "aliases:\n",
    "- /2025/08/26/SparkSnowflake\n",
    "badges: true\n",
    "categories:\n",
    "- spark\n",
    "- snowflake\n",
    "date: '2025-08-26'\n",
    "description: Details on how to run Apache Spark in Snowflake.\n",
    "output-file: 2025-08-26-sparkinSnowflake.html\n",
    "title: Running Apache Spark in Snowflake\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a00a7-4030-497d-866b-a4e293881463",
   "metadata": {
    "collapsed": false,
    "name": "topic"
   },
   "source": "# Running Apache Spark in Snowflake\n\n\n\n\nDataset: https://health.data.ny.gov/api/views/jxy9-yhdk/rows.csv\n\n\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "313d3a0c-65c1-4625-8ac5-854f6d1b1991",
   "metadata": {
    "name": "image",
    "collapsed": false
   },
   "source": "![image-20240507155528488](./images/image-sparksnowflake.png)\n"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a59d27c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'snowflake_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m      8\u001b[39m load_dotenv()\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Connection parameters - Update these with your Snowflake credentials\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# connection_parameters = {\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#     'account': os.getenv('SNOWFLAKE_ACCOUNT', 'your_account_identifier'),\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Alternative: Import from config file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msnowflake_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SNOWFLAKE_CONFIG\n\u001b[32m     23\u001b[39m connection_parameters = SNOWFLAKE_CONFIG\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# Create Snowpark session\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'snowflake_config'"
     ]
    }
   ],
   "source": [
    "# Snowflake Connection Setup for Cursor/Jupyter\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from snowflake.snowpark import Session\n",
    "# from snowflake import snowpark_connect  # Not available in standard installations\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Connection parameters - Update these with your Snowflake credentials\n",
    "# connection_parameters = {\n",
    "#     'account': os.getenv('SNOWFLAKE_ACCOUNT', 'your_account_identifier'),\n",
    "#     'user': os.getenv('SNOWFLAKE_USER', 'your_username'), \n",
    "#     'password': os.getenv('SNOWFLAKE_PASSWORD', 'your_password'),\n",
    "#     'role': os.getenv('SNOWFLAKE_ROLE', 'SYSADMIN'),\n",
    "#     'warehouse': os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n",
    "#     'database': os.getenv('SNOWFLAKE_DATABASE', 'AICOLLEGE'),\n",
    "#     'schema': os.getenv('SNOWFLAKE_SCHEMA', 'PUBLIC')\n",
    "# }\n",
    "\n",
    "# Alternative: Import from config file\n",
    "from snowflake_config import SNOWFLAKE_CONFIG\n",
    "connection_parameters = SNOWFLAKE_CONFIG\n",
    "\n",
    "try:\n",
    "    # Create Snowpark session\n",
    "    session = Session.builder.configs(connection_parameters).create()\n",
    "    \n",
    "    # Note: We'll use Snowpark DataFrames instead of Spark DataFrames\n",
    "    # Snowpark provides similar functionality to Spark for data processing\n",
    "    \n",
    "    print(\"✅ Connected to Snowflake successfully!\")\n",
    "    print(f\"Current database: {session.get_current_database()}\")\n",
    "    print(f\"Current schema: {session.get_current_schema()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Connection failed: {str(e)}\")\n",
    "    print(\"Please check your Snowflake credentials in snowflake_config.py or .env file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af9254-da40-4e5a-9525-8d3a34b379ea",
   "metadata": {
    "language": "python",
    "name": "setup"
   },
   "outputs": [],
   "source": "# Set up the env for Java libraries and enable the Spark Connect Mode\nimport os\nimport traceback\n\nos.environ['JAVA_HOME'] = os.environ[\"CONDA_PREFIX\"]\nos.environ['JAVA_LD_LIBRARY_PATH'] = os.path.join(os.environ[\"CONDA_PREFIX\"], 'lib', 'server')\nos.environ[\"SPARK_LOCAL_HOSTNAME\"] = \"127.0.0.1\"\nos.environ[\"SPARK_CONNECT_MODE_ENABLED\"] = \"1\"\n\nfrom snowflake import snowpark_connect\nfrom snowflake.snowpark.context import get_active_session\n\n\nsession = get_active_session()\nsnowpark_connect.start_session(snowpark_session = session)\n\n\n# Here is your normal pyspark code. You can of course have them in other Python Cells\nspark = snowpark_connect.get_session()\ndf = spark.sql(\"show schemas\").limit(10)\ndf.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc925d72-924d-4f3b-aca9-9fbdb0cf7b9f",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "create_dataframe"
   },
   "outputs": [],
   "source": "# Create a DataFrame with sample data using Snowpark\nfrom snowflake.snowpark.types import StructType, StructField, IntegerType, StringType\n\ndata = [[2021, \"test\", \"Albany\", \"M\", 42]]\nschema = StructType([\n    StructField(\"Year\", IntegerType()),\n    StructField(\"First Name\", StringType()),\n    StructField(\"County\", StringType()),\n    StructField(\"Sex\", StringType()),\n    StructField(\"Count\", IntegerType())\n])\n\ndf1 = session.create_dataframe(data, schema=schema)\ndf1.show()  # Snowpark DataFrame show() method\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82578820-f654-411d-91e2-ca422fc610dd",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "load_csv"
   },
   "outputs": [],
   "source": [
    "# Read CSV file using Snowpark\n",
    "from snowflake.snowpark.types import *\n",
    "\n",
    "# Option 1: Read from stage using Snowpark\n",
    "df_csv = session.read.option(\"FIELD_DELIMITER\", \",\").option(\"SKIP_HEADER\", 1).csv(\"@aicollege.public.setup/row.csv\")\n",
    "\n",
    "# Option 2: If the CSV is already loaded as a table, use SQL\n",
    "# df_csv = session.table(\"your_table_name\")\n",
    "\n",
    "df_csv.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d1b48-c43d-4074-b4c7-a31e6277d917",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "interact_with_dataframe"
   },
   "outputs": [],
   "source": [
    "# Print schema information for both DataFrames\n",
    "print(\"CSV DataFrame schema:\")\n",
    "df_csv.show(1)  # Show first row to see column structure\n",
    "print(\"\\nSample DataFrame schema:\")\n",
    "df1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972727b-6c8d-4f6d-969c-51e19a4a4bfa",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "rename"
   },
   "outputs": [],
   "source": [
    "# Rename column using Snowpark\n",
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "df_csv = df_csv.with_column_renamed(\"First Name\", \"First_Name\")\n",
    "print(\"After renaming column:\")\n",
    "df_csv.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab83992-8170-490e-9632-3408d172d8a1",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "combine"
   },
   "outputs": [],
   "source": [
    "# Union DataFrames using Snowpark\n",
    "df = df1.union_all(df_csv)  # union_all is the Snowpark equivalent\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3644da-d810-4680-ad0d-b9d580bfa016",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "filter"
   },
   "outputs": [],
   "source": [
    "# Filter using Snowpark\n",
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "df.filter(col(\"Count\") > 50).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb7728-6dd4-4c5a-85cf-902903d82b9f",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "where"
   },
   "outputs": [],
   "source": [
    "df.where(df[\"Count\"] > 50).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f5f38-bf7f-49ad-b7da-a4a98cc2c770",
   "metadata": {
    "language": "python",
    "name": "select_column",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from pyspark.sql.functions import desc\ndf.select(\"First_Name\", \"Count\").orderBy(desc(\"Count\")).show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f848a-082b-4237-94ae-8209fd27abf6",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "subset_df"
   },
   "outputs": [],
   "source": [
    "subsetDF = df.filter((df[\"Year\"] == 2009) & (df[\"Count\"] > 100) & (df[\"Sex\"] == \"F\")).select(\"First_Name\", \"County\", \"Count\").orderBy(desc(\"Count\"))\n",
    "subsetDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88964d4a-e3a7-400e-ac7e-f1b5de990077",
   "metadata": {
    "language": "python",
    "name": "save_df",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"AICOLLEGE.PUBLIC.MYFIRSTSPARK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98171cb6-6e9c-4adf-8248-bae1a4638068",
   "metadata": {
    "language": "python",
    "name": "save_to_json",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "df.write.format(\"json\").mode(\"overwrite\").save(f\"@aicollege.public.setup/myfirstspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10945fd0-e03e-4f4e-a172-5b62055c6e74",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "read_json"
   },
   "outputs": [],
   "source": [
    "#spark.read.format(\"json\").json(f\"@aicollege.public.setup/myfirstspark\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bac19c-6ab6-40e7-bbb7-b46cbab0d2c8",
   "metadata": {
    "language": "python",
    "name": "selectExpr"
   },
   "outputs": [],
   "source": [
    "df.selectExpr(\"Count\", \"upper(County) as big_name\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7565d0-1708-4f33-918f-f6d2669e359b",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "expr"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df.select(\"Count\", expr(\"lower(County) as little_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af7cf4-4023-42b0-acb9-4cd11189c80e",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "spark_sql"
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM AICOLLEGE.PUBLIC.MYFIRSTSPARK\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ab187-598b-4f32-9456-b819c315ab51",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": []
  }
 ]
}
