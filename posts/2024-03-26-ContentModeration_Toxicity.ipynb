{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Moderation - Toxicity Classification\n",
    "\n",
    "This notebook will capture different methods of performing text moderation, in special with focus on toxicity classification.\n",
    "It will be divided into 3 parts:\n",
    "- Part 1: Text Moderation - Toxicity Classification using Amazon Comprehend API\n",
    "- Part 2: Text Moderation - Toxicity Classification using Amazon Comprehend Custom Model\n",
    "- Part 3: Text Moderation - Toxicity Classification using Large Language Models\n",
    "\n",
    "\n",
    "This notebook is adapted from the following sources:\n",
    "- [aws-samples/amazon-rekognition-code-samples](https://github.com/aws-samples/amazon-rekognition-code-samples/blob/main/content-moderation/04-text-moderation/02-content-moderation-text-toxic-classification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Moderation - Toxicity Classification using Amazon Comprehend API\n",
    "\n",
    "Amazon Comprehend has the capability to perform [toxicity analysis](https://docs.aws.amazon.com/comprehend/latest/dg/trust-safety.html) through the API [detect_toxic_content](https://docs.aws.amazon.com/comprehend/latest/APIReference/API_DetectToxicContent.html).\n",
    "\n",
    "detect_toxic_content performs toxicity analysis on the list of text strings that you provide as input. The API response contains a results list that matches the size of the input list. \n",
    "\n",
    "The toxicity content labels are:\n",
    "GRAPHIC | HARASSMENT_OR_ABUSE | HATE_SPEECH | INSULT | PROFANITY | SEXUAL | VIOLENCE_OR_THREAT\n",
    "\n",
    "The response syntax for the API call is show below.\n",
    "\n",
    "\n",
    "`\n",
    "json\n",
    "{\n",
    "   \"ResultList\": [ \n",
    "      { \n",
    "         \"Labels\": [ \n",
    "            { \n",
    "               \"Name\": \"string\",\n",
    "               \"Score\": number\n",
    "            }\n",
    "         ],\n",
    "         \"Toxicity\": number\n",
    "      }\n",
    "   ]\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Labels': [{'Name': 'PROFANITY', 'Score': 0.046799998730421066}, {'Name': 'HATE_SPEECH', 'Score': 0.056699998676776886}, {'Name': 'INSULT', 'Score': 0.10109999775886536}, {'Name': 'GRAPHIC', 'Score': 0.01860000006854534}, {'Name': 'HARASSMENT_OR_ABUSE', 'Score': 0.08330000191926956}, {'Name': 'SEXUAL', 'Score': 0.07109999656677246}, {'Name': 'VIOLENCE_OR_THREAT', 'Score': 0.03519999980926514}], 'Toxicity': 0.08330000191926956}, {'Labels': [{'Name': 'PROFANITY', 'Score': 0.353300005197525}, {'Name': 'HATE_SPEECH', 'Score': 0.15479999780654907}, {'Name': 'INSULT', 'Score': 0.2046000063419342}, {'Name': 'GRAPHIC', 'Score': 0.0812000036239624}, {'Name': 'HARASSMENT_OR_ABUSE', 'Score': 0.12559999525547028}, {'Name': 'SEXUAL', 'Score': 0.16859999299049377}, {'Name': 'VIOLENCE_OR_THREAT', 'Score': 0.08150000125169754}], 'Toxicity': 0.15479999780654907}, {'Labels': [{'Name': 'PROFANITY', 'Score': 0.353300005197525}, {'Name': 'HATE_SPEECH', 'Score': 0.15479999780654907}, {'Name': 'INSULT', 'Score': 0.2046000063419342}, {'Name': 'GRAPHIC', 'Score': 0.0812000036239624}, {'Name': 'HARASSMENT_OR_ABUSE', 'Score': 0.12559999525547028}, {'Name': 'SEXUAL', 'Score': 0.16859999299049377}, {'Name': 'VIOLENCE_OR_THREAT', 'Score': 0.08150000125169754}], 'Toxicity': 0.15479999780654907}, {'Labels': [{'Name': 'PROFANITY', 'Score': 0.6852999925613403}, {'Name': 'HATE_SPEECH', 'Score': 0.5633000135421753}, {'Name': 'INSULT', 'Score': 0.968999981880188}, {'Name': 'GRAPHIC', 'Score': 0.07450000196695328}, {'Name': 'HARASSMENT_OR_ABUSE', 'Score': 0.2046000063419342}, {'Name': 'SEXUAL', 'Score': 0.26249998807907104}, {'Name': 'VIOLENCE_OR_THREAT', 'Score': 0.10050000250339508}], 'Toxicity': 0.9890000224113464}]\n",
      "Text segment 2\n",
      "PROFANITY score 0.35\n",
      "INSULT score 0.20\n",
      "Text segment 3\n",
      "PROFANITY score 0.35\n",
      "INSULT score 0.20\n",
      "Text segment 4\n",
      "PROFANITY score 0.69\n",
      "HATE_SPEECH score 0.56\n",
      "INSULT score 0.97\n",
      "HARASSMENT_OR_ABUSE score 0.20\n",
      "SEXUAL score 0.26\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "\n",
    "# Seeting variables\n",
    "boto3session = boto3.Session(profile_name='marcasbr+genai-Admin')\n",
    "region = boto3session.region_name\n",
    "comprehendrole = boto3session.client('iam').get_role(RoleName='AmazonComprehendServiceRole-access-role')['Role']['Arn']\n",
    "comprehend = boto3session.client('comprehend', region_name=region)\n",
    "\n",
    "THRESHOLD = 0.2\n",
    "response = comprehend.detect_toxic_content(\n",
    "    TextSegments=[\n",
    "        {\n",
    "            \"Text\": \"You can go through the door go, he's waiting for you on the right.\"\n",
    "        },\n",
    "        {\n",
    "            \"Text\": \"***\"\n",
    "        },\n",
    "        {\n",
    "            \"Text\": \"***\"\n",
    "        },\n",
    "        {\n",
    "            \"Text\": \"Elon March is a piece of shit, greedy capitalis\"\n",
    "        }\n",
    "        \n",
    "    ],\n",
    "    LanguageCode='en'\n",
    ")\n",
    "\n",
    "result_list = response['ResultList']\n",
    "\n",
    "print(result_list)\n",
    "\n",
    "for i, result in enumerate(result_list):\n",
    "    labels = result['Labels']\n",
    "    detected = [ l for l in labels if l['Score'] > THRESHOLD ]\n",
    "    if len(detected) > 0:\n",
    "        print(\"Text segment {}\".format(i + 1))\n",
    "        for d in detected:\n",
    "            print(\"{} score {:.2f}\".format(d['Name'], d['Score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Moderation - Toxicity Classification using Amazon Comprehend Custom Model\n",
    "\n",
    "The threat of abuse and harassment online means that many people stop expressing themselves and give up on seeking different opinions. Platforms struggle to effectively facilitate conversations, leading many communities to limit or completely shut down user comments. \n",
    "\n",
    "Toxicity classification allows customers from Gaming, Social Media, and many other industries automatically classify the user-generated text content and filter out the toxic ones to keep the online environment inclusive.\n",
    "\n",
    "In this Lab, we will use an AWS AI service - [Comprehend Custom Classfication](https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html) feature to train a custom model to classify toxicity text messages.\n",
    "\n",
    "![Arch](./images/text-moderation-toxic-classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxicity Classficiation\n",
    "\n",
    "- [Step 1: Setup notebook](#step1)\n",
    "- [Step 2: Prepare custom classification training dataset](#step2)\n",
    "- [Step 3: Create Amazon Comprehend Classification training job](#step3)\n",
    "- [Step 4: Create Amazon Comprehend real time endpoint](#step4)\n",
    "- [Step 5: Classify Documents using the real-time endpoint](#step5)\n",
    "- [Step 6: Classify message using the real-time endpoint and capture metrics accross a random selection of texts](#step6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Setup Notebook <a id=\"step1\"></a>\n",
    "Run the below cell to install/update Python dependencies if you run the lab using a local IDE. It is optional if you use a SageMaker Studio Juypter Notebook, which already includes the dependencies in the kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# First, let's get the latest installations of our dependencies\n",
    "%pip install -qU pip\n",
    "%pip install boto3 -qU\n",
    "%pip install sagemaker -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can skip the below cell if you are using SageMaker Studio Data Science kernel or they are already installed in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install pandas if you are using a local IDE and they are not installed in your env\n",
    "%pip install pandas -qU\n",
    "%pip install datetime -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker role is: arn:aws:iam::754992829378:role/Admin\n",
      "Default SageMaker Bucket: s3://sagemaker-us-east-1-754992829378\n",
      "Comprehend role is: arn:aws:iam::754992829378:role/AmazonComprehendServiceRole-access-role\n",
      "AWS region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker as sm\n",
    "import os\n",
    "import io\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# variables\n",
    "boto3session = boto3.Session(profile_name='marcasbr+genai-Admin')\n",
    "smsession = sm.Session(boto_session=boto3session)\n",
    "data_bucket = sm.Session(boto_session=boto3session).default_bucket()\n",
    "region = boto3session.region_name\n",
    "\n",
    "os.environ[\"BUCKET\"] = data_bucket\n",
    "os.environ[\"REGION\"] = region\n",
    "role = sm.session.get_execution_role(sagemaker_session=smsession)\n",
    "\n",
    "# Get the Comprehend service role ARN. For help check https://docs.aws.amazon.com/comprehend/latest/dg/tutorial-reviews-create-role.html\n",
    "comprehendrole = boto3session.client('iam').get_role(RoleName='AmazonComprehendServiceRole-access-role')['Role']['Arn']\n",
    "\n",
    "\n",
    "print(f\"SageMaker role is: {role}\\nDefault SageMaker Bucket: s3://{data_bucket}\")\n",
    "print(f\"Comprehend role is: {comprehendrole}\")\n",
    "print(f\"AWS region: {region}\")\n",
    "\n",
    "\n",
    "\n",
    "s3 = boto3session.client('s3')\n",
    "comprehend = boto3session.client('comprehend', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Prepare custom classification training dataset <a id=\"step2\"></a>\n",
    "Unzip the sample data **toxicity.zip** and decompress files to a local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./datasets/toxicity.zip\n",
      "replace toxicity_dataset/LICENSE? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!unzip ./datasets/toxicity.zip -d toxicity_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CSV file contains 500 toxic and 500 non-toxic comments from a variety of popular social media platforms. Click on toxicity_en.csv to see a spreadsheet of 1000 English examples.\n",
    "\n",
    "Columns:\n",
    "- text: the text of the comment\n",
    "- is_toxic: whether or not the comment is toxic\n",
    "\n",
    "(The dataset contained in **../datasets/toxicity.zip** is an unaltered redistribution of [the toxicity dataset](https://github.com/surge-ai/toxicity) made available by Surge AI under MIT License.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elon Musk is a piece of shit, greedy capitalis...</td>\n",
       "      <td>Toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The senile credit card shrill from Delaware ne...</td>\n",
       "      <td>Toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He does that a lot -- makes everyone look good...</td>\n",
       "      <td>Toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F*ck Lizzo</td>\n",
       "      <td>Toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Epstein and trump were best buds!!! Pedophiles...</td>\n",
       "      <td>Toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>My maternal abuelita taught me how to make pla...</td>\n",
       "      <td>Not Toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Funnily enough I was looking online last week ...</td>\n",
       "      <td>Not Toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>I can't bear how nice this is.\\n \\n I guess it...</td>\n",
       "      <td>Not Toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Going to buy a share of Tesla just to ensure i...</td>\n",
       "      <td>Not Toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>I only saw a couple of these throughout the mo...</td>\n",
       "      <td>Not Toxic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text   is_toxic\n",
       "0    Elon Musk is a piece of shit, greedy capitalis...      Toxic\n",
       "1    The senile credit card shrill from Delaware ne...      Toxic\n",
       "2    He does that a lot -- makes everyone look good...      Toxic\n",
       "3                                           F*ck Lizzo      Toxic\n",
       "4    Epstein and trump were best buds!!! Pedophiles...      Toxic\n",
       "..                                                 ...        ...\n",
       "995  My maternal abuelita taught me how to make pla...  Not Toxic\n",
       "996  Funnily enough I was looking online last week ...  Not Toxic\n",
       "997  I can't bear how nice this is.\\n \\n I guess it...  Not Toxic\n",
       "998  Going to buy a share of Tesla just to ensure i...  Not Toxic\n",
       "999  I only saw a couple of these throughout the mo...  Not Toxic\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./toxicity_dataset/toxicity_en.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use this dataset to train a Comprehend Custom Classification model to classify toxic sentences.\n",
    "Comprehend custom classification supports 2 modes: [multi-class](https://docs.aws.amazon.com/comprehend/latest/dg/prep-classifier-data-multi-class.html) or [multi-label](https://docs.aws.amazon.com/comprehend/latest/dg/prep-classifier-data-multi-label.html). Comprehend multi-class mode accepts training datasets in 2 formats: CSV or Augmented manifest file. In this lab, we will train a model in the multi-class mode with the training dataset in CSV format. \n",
    "\n",
    "For more information, refer to this [doc](https://docs.aws.amazon.com/comprehend/latest/dg/prep-classifier-data-multi-class.html) for more details about the multi-class data format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprehend custom classifiers requires the CSV's first column to be the label and the second column to be the text. The CSV file doesn't require a header. The below code will create a CSV file in the expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('toxicity-custom-classification.csv', header=False, index=False, columns=['is_toxic','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's upload the training data to the S3 bucket, ready for Comprehend to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_key = 'content-moderation-im/text-moderation/toxicity-custom-classification.csv'\n",
    "s3.upload_file(f'toxicity-custom-classification.csv', data_bucket, s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create Amazon Comprehend Classification training job <a id=\"step3\"></a>\n",
    "Once we have a labeled dataset ready we are going to create and train a [Amazon Comprehend custom classification model](https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html) with the dataset.\n",
    "\n",
    "This job can take ~40 minutes to complete. Once the training job is completed move on to next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classifier with the name \"Sample-Toxicity-Classifier-Content-Moderation\" already exists.\n",
      "The classifier ARN is: \"arn:aws:comprehend:us-east-1:754992829378:document-classifier/Sample-Toxicity-Classifier-Content-Moderation/version/v5\"\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Create a Toxicity classifier\n",
    "account_id = boto3session.client('sts').get_caller_identity().get('Account')\n",
    "id = str(datetime.datetime.now().strftime(\"%s\"))\n",
    "\n",
    "document_classifier_name = 'Sample-Toxicity-Classifier-Content-Moderation'\n",
    "document_classifier_version = 'v5'\n",
    "document_classifier_arn = ''\n",
    "response = None\n",
    "\n",
    "try:\n",
    "    create_response = comprehend.create_document_classifier(\n",
    "        InputDataConfig={\n",
    "            'DataFormat': 'COMPREHEND_CSV',\n",
    "            'S3Uri': f's3://{data_bucket}/{s3_key}'\n",
    "        },\n",
    "        DataAccessRoleArn=comprehendrole,\n",
    "        DocumentClassifierName=document_classifier_name,\n",
    "        VersionName=document_classifier_version,\n",
    "        LanguageCode='en',\n",
    "        Mode='MULTI_CLASS'\n",
    "    )\n",
    "    \n",
    "    document_classifier_arn = create_response['DocumentClassifierArn']\n",
    "    \n",
    "    print(f\"Comprehend Custom Classifier created with ARN: {document_classifier_arn}\")\n",
    "except Exception as error:\n",
    "    if error.response['Error']['Code'] == 'ResourceInUseException':\n",
    "        print(f'A classifier with the name \"{document_classifier_name}\" already exists.')\n",
    "        document_classifier_arn = f'arn:aws:comprehend:{region}:{account_id}:document-classifier/{document_classifier_name}/version/{document_classifier_version}'\n",
    "        print(f'The classifier ARN is: \"{document_classifier_arn}\"')\n",
    "    else:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check status of the Comprehend Custom Classification Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:56:52 : Custom document classifier: TRAINED\n",
      "CPU times: user 1.72 s, sys: 662 ms, total: 2.38 s\n",
      "Wall time: 34min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loop through and wait for the training to complete . Takes up to 10 mins \n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "jobArn = create_response['DocumentClassifierArn']\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    describe_custom_classifier = comprehend.describe_document_classifier(\n",
    "        DocumentClassifierArn = jobArn\n",
    "    )\n",
    "    status = describe_custom_classifier[\"DocumentClassifierProperties\"][\"Status\"]\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{current_time} : Custom document classifier: {status}\")\n",
    "    \n",
    "    if status == \"TRAINED\" or status == \"IN_ERROR\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, to create a Comprehend Custom Classifier Job manually using the console go to Amazon Comprehend Console\n",
    "- On the left menu click \"Custom Classification\"\n",
    "- In the \"Classifier models\" section, click on \"Create new model\"\n",
    "- In Model Setting for Model name, enter a name \n",
    "- In Data Specification; select \"Using Single-label\" mode and for Data format select CSV file\n",
    "- For Training dataset browse to your data-bucket created above and select the file toxicity-custom-classification.csv\n",
    "- For IAM role select \"Create an IAM role\" and specify a prefix (this will create a new IAM Role for Comprehend)\n",
    "- Click create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create Amazon Comprehend real time endpoint <a id=\"step4\"></a>\n",
    "Once our Comprehend custom classifier is fully trained (i.e. status = TRAINED). We can create a real-time endpoint. We will use this endpoint to classify text inputs in real time. The following code cells use the comprehend Boto3 client to create an endpoint, but you can also create one manually via the console. Instructions on how to do that can be found in the subsequent section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint created with ARN: arn:aws:comprehend:us-east-1:754992829378:document-classifier-endpoint/toxicity-endpoint\n"
     ]
    }
   ],
   "source": [
    "#create comprehend endpoint\n",
    "model_arn = document_classifier_arn\n",
    "ep_name = 'toxicity-endpoint'\n",
    "\n",
    "try:\n",
    "    endpoint_response = comprehend.create_endpoint(\n",
    "        EndpointName=ep_name,\n",
    "        ModelArn=model_arn,\n",
    "        DesiredInferenceUnits=1,    \n",
    "        DataAccessRoleArn=comprehendrole\n",
    "    )\n",
    "    ENDPOINT_ARN=endpoint_response['EndpointArn']\n",
    "    print(f'Endpoint created with ARN: {ENDPOINT_ARN}')    \n",
    "except Exception as error:\n",
    "    if error.response['Error']['Code'] == 'ResourceInUseException':\n",
    "        print(f'An endpoint with the name \"{ep_name}\" already exists.')\n",
    "        ENDPOINT_ARN = f'arn:aws:comprehend:{region}:{account_id}:document-classifier-endpoint/{ep_name}'\n",
    "        print(f'The classifier endpoint ARN is: \"{ENDPOINT_ARN}\"')\n",
    "        %store ENDPOINT_ARN\n",
    "    else:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:comprehend:us-east-1:754992829378:document-classifier-endpoint/toxicity-endpoint',\n",
       " 'ResponseMetadata': {'RequestId': '375b1283-0960-46d6-93a6-7cf64927b4db',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '375b1283-0960-46d6-93a6-7cf64927b4db',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '106',\n",
       "   'date': 'Wed, 27 Mar 2024 10:10:02 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(endpoint_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, use the steps below to create a Comprehend endpoint using the AWS console.\n",
    "- Go to Comprehend on AWS Console and click on Endpoints in the left menu.\n",
    "- Click on \"Create endpoint\"\n",
    "- Give an Endpoint name; for Custom model type select Custom classification; for version select no version or the latest version of the model.\n",
    "- For Classifier model select from the drop down menu\n",
    "- For Inference Unit select 1\n",
    "- Check \"Acknowledge\"\n",
    "- Click \"Create endpoint\"\n",
    "\n",
    "[It may take ~10 minutes](https://console.aws.amazon.com/comprehend/v2/home?region=us-east-1#endpoints) for the endpoint to get created. The code cell below checks the creation status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:16:22 : Custom document classifier: IN_SERVICE\n",
      "CPU times: user 26.5 ms, sys: 8.7 ms, total: 35.2 ms\n",
      "Wall time: 533 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loop through and wait for the training to complete . Takes up to 10 mins \n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "ep_arn = endpoint_response[\"EndpointArn\"]\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    describe_endpoint_resp = comprehend.describe_endpoint(\n",
    "        EndpointArn=ep_arn\n",
    "    )\n",
    "    status = describe_endpoint_resp[\"EndpointProperties\"][\"Status\"]\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{current_time} : Custom document classifier: {status}\")\n",
    "    \n",
    "    if status == \"IN_SERVICE\" or status == \"FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Classify message using the real-time endpoint <a id=\"step5\"></a>\n",
    "\n",
    "Once the endpoint has been created, we will use some sample text messages to classify them into toxic or non-toxic categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Classes': [{'Name': 'Toxic', 'Score': 0.9951152801513672},\n",
       "  {'Name': 'Not Toxic', 'Score': 0.004884751979261637}],\n",
       " 'ResponseMetadata': {'RequestId': '60078846-5eda-4b85-a6be-8be582564703',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '60078846-5eda-4b85-a6be-8be582564703',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '107',\n",
       "   'date': 'Wed, 27 Mar 2024 10:16:25 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = comprehend.classify_document(\n",
    "    Text= 'Why don''t you shoot him?! I hate you all!',\n",
    "    EndpointArn=ENDPOINT_ARN\n",
    ")\n",
    "display(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference result returned by Comprehend endpoint contains a \"Classes\" node, a list of labeled classes with a 'Score' representing the confidence score of the inference result.\n",
    "\n",
    "The above response shows that the text message \"Why don't you shoot him?! I hate you all!\" has a high confidence score (> 99%) for the \"Toxic\" category. You can try different inputs to test the Toxicity classification result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Classify message using the real-time endpoint <a id=\"step6\"></a> and capture metrics accross a random selection of texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vt/7n9vmpnn1sg9xzf_2z58706r0000gr/T/ipykernel_19577/1421068623.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  my_df = pd.concat([my_df, df1])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>ground_thruth</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Score</th>\n",
       "      <th>Content Length</th>\n",
       "      <th>elapsed_time_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Right!! I was already excited for X Choice, &amp; ...</td>\n",
       "      <td>Not Toxic</td>\n",
       "      <td>Not Toxic</td>\n",
       "      <td>0.999895</td>\n",
       "      <td>107</td>\n",
       "      <td>1.273570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If I learned anything is those teachers are li...</td>\n",
       "      <td>Not Toxic</td>\n",
       "      <td>Not Toxic</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>107</td>\n",
       "      <td>0.197692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Save them babies from their murderous mothers</td>\n",
       "      <td>Toxic</td>\n",
       "      <td>Toxic</td>\n",
       "      <td>0.999855</td>\n",
       "      <td>108</td>\n",
       "      <td>2.644358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOCH DemoCraps!!! \"LET'S GO BRANDON\"</td>\n",
       "      <td>Toxic</td>\n",
       "      <td>Toxic</td>\n",
       "      <td>0.999886</td>\n",
       "      <td>108</td>\n",
       "      <td>0.234662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If you build ap with lethal tempo on him you c...</td>\n",
       "      <td>Not Toxic</td>\n",
       "      <td>Not Toxic</td>\n",
       "      <td>0.999921</td>\n",
       "      <td>107</td>\n",
       "      <td>0.270135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text ground_thruth Prediction  \\\n",
       "0  Right!! I was already excited for X Choice, & ...     Not Toxic  Not Toxic   \n",
       "0  If I learned anything is those teachers are li...     Not Toxic  Not Toxic   \n",
       "0      Save them babies from their murderous mothers         Toxic      Toxic   \n",
       "0               FOCH DemoCraps!!! \"LET'S GO BRANDON\"         Toxic      Toxic   \n",
       "0  If you build ap with lethal tempo on him you c...     Not Toxic  Not Toxic   \n",
       "\n",
       "      Score Content Length  elapsed_time_sec  \n",
       "0  0.999895            107          1.273570  \n",
       "0  0.999918            107          0.197692  \n",
       "0  0.999855            108          2.644358  \n",
       "0  0.999886            108          0.234662  \n",
       "0  0.999921            107          0.270135  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def display(text, ground_thruth):\n",
    "    start = time.time()\n",
    "    response = comprehend.classify_document(\n",
    "        Text= text,\n",
    "        EndpointArn=ENDPOINT_ARN\n",
    "    )\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    \n",
    "    #Building the dataframe\n",
    "    scores = [item['Score'] for item in response['Classes']]\n",
    "    max_index = scores.index(max(scores))\n",
    "    result = response['Classes'][max_index]\n",
    "    name = result['Name']\n",
    "    score = max(scores)\n",
    "    content_length = response['ResponseMetadata']['HTTPHeaders'].get('content-length')\n",
    "    result = {'Text': text[:50], 'ground_thruth': ground_thruth, 'Prediction': name, 'Score': score, 'Content Length': content_length, 'elapsed_time_sec': elapsed}\n",
    "    return result\n",
    "\n",
    "# Get random samples from dataset\n",
    "nsel = 5\n",
    "df = pd.read_csv('./toxicity_dataset/toxicity_en.csv')\n",
    "df_selection = df.sample(n=nsel).head(nsel)\n",
    "my_df = pd.DataFrame(columns=['Text', 'ground_thruth', 'Prediction', 'Score', 'Content Length', 'elapsed_time_sec'])\n",
    "\n",
    "for index, row in df_selection.iterrows():\n",
    "  text = row['text']\n",
    "  ground_thruth = row['is_toxic']\n",
    "  df1 = pd.DataFrame(display(text, ground_thruth), index=[0])\n",
    "  my_df = pd.concat([my_df, df1]) \n",
    "    \n",
    "my_df.head(nsel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "Cleanup is optional if you want to execute subsequent notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Comprehend Endpoint\n",
    "resp = comprehend.delete_endpoint(EndpointArn=ENDPOINT_ARN)\n",
    "display(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to wait a few minutes to run the below cell until the Comprehend endpoint is deleted successfully and the classifier is no longer in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Comprehend Custom Classifier \n",
    "resp = comprehend.delete_document_classifier(DocumentClassifierArn=document_classifier_arn)\n",
    "display(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this lab, we have trained an Amazon Comprehend custom classifier using a sample toxicity dataset. And deploy the Custom Classifier to a Comprehend endpoint to serve real-time inference. "
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
