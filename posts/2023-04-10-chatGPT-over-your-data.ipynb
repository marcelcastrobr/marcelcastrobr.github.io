{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (1482697714.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[37], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    - /2023/04/10/ChatGPTovermydata\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
     ]
    }
   ],
   "source": [
    "---\n",
    "aliases:\n",
    "- /2023/04/10/ChatGPToverdata\n",
    "date: '2023-04-10'\n",
    "output-file: 2023-04-10-ChatGPTovermydata.html\n",
    "title: ChatGPT over your data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chatGPT over your data\n",
    "\n",
    "Notebook inspired by:\n",
    "- [Tutorial: ChatGPT Over Your Data](https://blog.langchain.dev/tutorial-chatgpt-over-your-data/)\n",
    "- [Build a GitHub Support Bot with GPT3, LangChain, and Python](https://dagster.io/blog/chatgpt-langchain#our-problem)\n",
    "- [Meet Bricky - a conversational bot using OpenAI](https://github.com/larsbaunwall/bricky)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing packages needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# update or install the necessary libraries\n",
    "!pip install --upgrade openai\n",
    "!pip install --upgrade langchain\n",
    "!pip install --upgrade python-dotenv\n",
    "!pip install --upgrade pypdf\n",
    "!pip install --upgrade faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# update or install the necessary libraries\n",
    "!pip install --upgrade openai\n",
    "!pip install --upgrade langchain\n",
    "!pip install --upgrade python-dotenv\n",
    "!pip install --upgrade pypdf\n",
    "!pip install --upgrade faiss-cpu\n",
    "!pip install --upgrade tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.docstore.document import Document\n",
    "import requests\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pathlib\n",
    "import subprocess\n",
    "import tempfile\n",
    "import pickle\n",
    "\n",
    "import openai\n",
    "import os\n",
    "import IPython\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# API configuration\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# for LangChain\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"SERPAPI_API_KEY\"] = os.getenv(\"SERPAPI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading markdown documents for github using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_github_docs(repo_owner, repo_name):\n",
    "    with tempfile.TemporaryDirectory() as d:\n",
    "        subprocess.check_call(\n",
    "            f\"git clone --depth 1 https://github.com/{repo_owner}/{repo_name}.git .\",\n",
    "            cwd=d,\n",
    "            shell=True,\n",
    "        )\n",
    "        git_sha = (\n",
    "            subprocess.check_output(\"git rev-parse HEAD\", shell=True, cwd=d)\n",
    "            .decode(\"utf-8\")\n",
    "            .strip()\n",
    "        )\n",
    "        repo_path = pathlib.Path(d)\n",
    "        markdown_files = list(repo_path.glob(\"**/*.md\")) + list(\n",
    "            repo_path.glob(\"**/*.mdx\")\n",
    "        )\n",
    "        for markdown_file in markdown_files:\n",
    "            with open(markdown_file, \"r\") as f:\n",
    "                relative_path = markdown_file.relative_to(repo_path)\n",
    "                github_url = f\"https://github.com/{repo_owner}/{repo_name}/blob/{git_sha}/{relative_path}\"\n",
    "                yield Document(page_content=f.read(), metadata={\"source\": github_url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def source_docs():\n",
    "    #return list(get_github_docs(\"dagster-io\", \"dagster\"))\n",
    "    #Sagemaker docs: awsdocs, amazon-sagemaker-developer-guide\n",
    "    return list(get_github_docs(\"awsdocs\", \"amazon-sagemaker-developer-guide\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search_index(source_docs):\n",
    "    source_chunks = []\n",
    "    splitter = CharacterTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=0)\n",
    "    for source in source_docs:\n",
    "        for chunk in splitter.split_text(source.page_content):\n",
    "            source_chunks.append(Document(page_content=chunk, metadata=source.metadata))\n",
    "\n",
    "    with open(\"search_index.pickle\", \"wb\") as f:\n",
    "        pickle.dump(FAISS.from_documents(source_chunks, OpenAIEmbeddings()), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = load_qa_with_sources_chain(OpenAI(temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_answer(question):\n",
    "    with open(\"search_index.pickle\", \"rb\") as f:\n",
    "        search_index = pickle.load(f)\n",
    "    print(\n",
    "        chain(\n",
    "            {\n",
    "                \"input_documents\": search_index.similarity_search(question, k=4),\n",
    "                \"question\": question,\n",
    "            },\n",
    "            return_only_outputs=True,\n",
    "        )[\"output_text\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '.'...\n",
      "Created a chunk of size 1056, which is longer than the specified 1024\n",
      "Created a chunk of size 1807, which is longer than the specified 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(search_index(source_docs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, you can use SageMaker for Training and Inference with Apache Spark.\n",
      "SOURCES: \n",
      "https://github.com/awsdocs/amazon-sagemaker-developer-guide/blob/d514c7799d1c934c96e97655b71dbd9cd78cd59b/doc_source/apache-spark.md\n",
      "https://github.com/awsdocs/amazon-sagemaker-developer-guide/blob/d514c7799d1c934c96e97655b71dbd9cd78cd59b/doc_source/how-it-works-prog-model.md\n"
     ]
    }
   ],
   "source": [
    "#print_answer(\"who is the lead singer of matchbox 20\")\n",
    "#print_answer(\"what are the types of sagemaker endpoints?\")\n",
    "print_answer(\"Can I use SageMaker for Training and Inference with Apache Spark?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loading from pdf files using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF contains 133 pages\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "filename = \"./example_data/2021-sustainability-report-amazon.pdf\"\n",
    "loader = PyPDFLoader(filename)\n",
    "pages = loader.load_and_split()\n",
    "print(f'PDF contains {len(pages)} pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search_index_pdf(source_docs):\n",
    "    source_chunks = []\n",
    "    splitter = CharacterTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=0)\n",
    "    for source in source_docs:\n",
    "        for chunk in splitter.split_text(source.page_content):\n",
    "            source_chunks.append(Document(page_content=chunk, metadata=source.metadata))\n",
    "\n",
    "    with open(\"search_index.pickle\", \"wb\") as f:\n",
    "        pickle.dump(FAISS.from_documents(source_chunks, OpenAIEmbeddings()), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(search_index_pdf(pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Amazon is aiming to reach net-zero carbon by 2030.\n",
      "SOURCES: 2021-sustainability-report-amazon.pdf\n"
     ]
    }
   ],
   "source": [
    "print_answer(\"When is Amazon net-zero carbon?\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f38e0373277d6f71ee44ee8fea5f1d408ad6999fda15d538a69a99a1665a839d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
