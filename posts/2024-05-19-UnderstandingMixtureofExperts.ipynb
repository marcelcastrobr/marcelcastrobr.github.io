{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed92a7e-adf6-4fb3-98c7-171d52178bd5",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /2024/05/19/UnderstandingMistureOfExperts\n",
    "badges: true\n",
    "categories:\n",
    "- sagemaker\n",
    "- transformers\n",
    "- NLP\n",
    "- MoE\n",
    "date: '2024-05-19'\n",
    "description: Details on Mixture of Experts and how to run it.\n",
    "output-file: 2024-05-19-UnderstandingMistureOfExperts.html\n",
    "title: Understanding Mixture of Experts\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2e63b-a77d-4aa3-9c9a-a71c99bf57c8",
   "metadata": {},
   "source": [
    "# Understanding Misture of Experts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7ecc2-8fca-4e70-8be1-b20dc36a0f08",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "Basic knowledge of Python.\n",
    "\n",
    "Access to Amazon SageMaker Jumpstart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668795f5-1743-44df-8a25-a62393790332",
   "metadata": {},
   "source": [
    "## What is Mixture of Experts\n",
    "\n",
    "Mixture of Experts (MoE) idea dates back to 2010, where it has been explored for example in SVMs and Gaussian Process (ref. [Learning Factored Representations in a Deep Mixture of Experts\n",
    "](https://arxiv.org/abs/1312.4314)). Lately is has incorporated in LSTM with the introduction of sparsity (i.e. to allow running only parts of the whole neural network) (ref. [Switch Transformers](https://arxiv.org/abs/2101.03961)).\n",
    "\n",
    "The general idea of MoE is to replicate certain model components many times while routing each input only to a small subset of those replicas (a.k.a. experts).\n",
    "MoEs achieve faster inference for same model quality at the expense of significatly higher memory cost as all replicated components (a.k.a. parameter) need to be loaded in memory.\n",
    "\n",
    "\n",
    "Mixture of Experts (MoE) consists of the main two elements:\n",
    "\n",
    "#### Sparse MoE layer\n",
    "Instead of using dense feed-forward network (FFN), MoE makes use of sparse MoE layers known as \"experts\". As show in picture below, each expert is a neural network.\n",
    "\n",
    "\n",
    "#### Router or gate network\n",
    "\n",
    "In an MoE, the router determines which tokens are sent to which experts. The router is complsed by the learned parameters and its pre-trained at the same time as the rest of the network.\n",
    "\n",
    "![moe](./images/moe.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### MoE Benefits:\n",
    "The conditional computation on MoE where parts of the network are active on a per-token basis, allow us to scale the size of the model without increasing the computation.\n",
    "\n",
    "### MoE Challenges:\n",
    "\n",
    "As described by [Mixture of Experts Explained by HuggingFace](https://huggingface.co/blog/moe), MoE comes with some challenges: \n",
    "\n",
    "- **Training**: MoEs enable significantly more compute-efficient pretraining, but theyâ€™ve historically struggled to generalize during fine-tuning, leading to overfitting.\n",
    "\n",
    "- **High memory requirement at inference**: Although a MoE might have many parameters, only some of them are used during inference. This leads to fast inference compared to a dense model with same number of parameters. But, ALL parameters need to be loaded in RAM, so memory requirements are high. This is a disadvantage for MoE in edge devices as memory size is restricted. \n",
    "\n",
    "### MoE - Future Works:\n",
    "\n",
    " [QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models](https://arxiv.org/pdf/2310.16795)) proposed a new compression framework called QMoE which uses *quantization* as a way to compress trillion-parameters MoEs to less than 1 bit per parameter. Basically quantization converst the parameters (a.k.a model weights) to lower numerical precision (e.g. going from 16bits - half precision to 4 bits per weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd852b54-fd3b-496b-8503-9840350c6dd9",
   "metadata": {},
   "source": [
    "## Misture of Experts Myths\n",
    "\n",
    "### Myth 1: There are 8 experts in Mixtral 8x7B\n",
    "\n",
    "Every transformer layer has 8 experts and they are permuted in each layer.\n",
    "Instead of 8 experts what we have is a 256 independent experts in total accross the layers (32 x 8). \n",
    "\n",
    "###  Myth 2: There are 56B parameters in Mixtral 8x7B\n",
    "\n",
    "In reallity there are not 56B (8x7B) but 46.7B as the gating and attention layers are shared among the experts. Thus each token will see 12.9B active parameters instead of 14B parameters.\n",
    "\n",
    "### Myth 3: Cost and amount of active parameters are proportional\n",
    "\n",
    "Mixtral 8x7B has fewer active parameters than Llama2 13B.\n",
    "But by having expert routing in MoE you have a higher communication cost as you need to send tokens to different experts. Thus the cost and amount of active parameters are NOT proportiona in MoE.\n",
    "\n",
    "Note that in MoE you can not program which token you send to which expert. Thus while gaining on performance/cost, the absolute cost is not proportional to the amount of active parameters.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5911d794-155b-4ab9-a109-eb524c39d3f0",
   "metadata": {},
   "source": [
    "## How to implement MoE in PyTorch\n",
    "\n",
    "ToBeDefine\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33773442-6959-44ca-a31c-80b57a840e45",
   "metadata": {},
   "source": [
    "## Deploy Mixtral 8x7B Instruct MoE using SageMaker Jumpstart\n",
    "\n",
    "This notebook is inspired by Amazon SageMaker Jumpstart notebooks, which uses SageMaker Python SDK to deploy Mixtral 8x7B text generation model.\n",
    "\n",
    "For a complete list of all pre-trained model in Amazon SageMaker Jumpstart please check: https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c46355-2f40-4d03-8863-fd58a075cf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e8a2f20-ed71-455c-9cd8-fb87730ae42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"huggingface-llm-mixtral-8x7b-instruct\"\n",
    "accept_eula = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0159171-3def-4f0d-89cc-9ba2cf343420",
   "metadata": {},
   "source": [
    "### Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7361e3-5092-4892-9cda-47d48dcef9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------"
     ]
    }
   ],
   "source": [
    "model = JumpStartModel(model_id=model_id)\n",
    "\n",
    "# By default sagemaker expects an ml.p4d.24xlarge instance (NVIDIA A100 - 8 GPUs and 320 GB memory).\n",
    "# Due to quota restriction decided to use ml.g5.12xlarge (NVIDIA A10g  - 4 GPUs and 96 GB memory)\n",
    "predictor = model.deploy(accept_eula=accept_eula, instance_type='ml.g5.12xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb66be-0405-4284-9dda-db5e624c336e",
   "metadata": {},
   "source": [
    "### Invoke the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e591a829-58a3-4c84-b9d2-5b4bd5822a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    'input': 'Simply put, the theory of relativity states that',\n",
    "    'parameters': {'max_new_tokens':64, 'top_p':0.9, 'temperature': 0.6}\n",
    "}\n",
    "predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e35588-a882-4d83-b978-0947debaef0e",
   "metadata": {},
   "source": [
    "## Conclusion / Remarks \n",
    "ToBeDefine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3449d1f2-1575-4538-b1a0-419d71934fde",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "- [Stanford CS25: V4 I Demystifying Mixtral of Experts by Albert Jiang](https://www.youtube.com/watch?v=RcJ1YXHLv5o)\n",
    "- [Mixtral of Experts, arxiv: 2401.04088](https://arxiv.org/abs/2401.04088)\n",
    "- [Deploying Mistral 7B with NeuronX on Inf2 through SageMaker LMI container and streaming outputs](https://github.com/ghimiresunil/LLM-PowerHouse-A-Curated-Guide-for-Large-Language-Models-with-Custom-Training-and-Inferencing/blob/main/example_codebase/Inf2%20LLM%20SM%20Deployment/mistral7b_inf2.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8560644-0af2-44d3-a534-fbec8f5d35f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
