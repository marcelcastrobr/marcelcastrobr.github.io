{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed92a7e-adf6-4fb3-98c7-171d52178bd5",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /2024/05/19/UnderstandingMistureOfExperts\n",
    "badges: true\n",
    "categories:\n",
    "- sagemaker\n",
    "- transformers\n",
    "- NLP\n",
    "- MoE\n",
    "date: '2024-05-19'\n",
    "description: Details on Mixture of Experts and how to run it.\n",
    "output-file: 2024-05-19-UnderstandingMistureOfExperts.html\n",
    "title: Understanding Mixture of Experts\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2e63b-a77d-4aa3-9c9a-a71c99bf57c8",
   "metadata": {},
   "source": [
    "# Understanding Misture of Experts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7ecc2-8fca-4e70-8be1-b20dc36a0f08",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668795f5-1743-44df-8a25-a62393790332",
   "metadata": {},
   "source": [
    "## What is Mixture of Experts\n",
    "\n",
    "![moe](./images/moe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd852b54-fd3b-496b-8503-9840350c6dd9",
   "metadata": {},
   "source": [
    "## Misture of Experts Myths\n",
    "\n",
    "### Myth 1: There are 8 experts in Mixtral 8x7B\n",
    "\n",
    "Every transformer layer has 8 experts and they are permuted in each layer.\n",
    "Instead of 8 experts what we have is a 256 independent experts in total accross the layers (32 x 8). \n",
    "\n",
    "###  Myth 2: There are 56B parameters in Mixtral 8x7B\n",
    "\n",
    "In reallity there are not 56B (8x7B) but 46.7B as the gating and attention layers are shared among the experts. Thus each token will see 12.9B active parameters instead of 14B parameters.\n",
    "\n",
    "### Myth 3: Cost and amount of active parameters are proportional\n",
    "\n",
    "Mixtral 8x7B has fewer active parameters than Llama2 13B.\n",
    "But by having expert routing in MoE you have a higher communication cost as you need to send tokens to different experts. Thus the cost and amount of active parameters are NOT proportiona in MoE.\n",
    "\n",
    "Note that in MoE you can not program which token you send to which expert. Thus while gaining on performance/cost, the absolute cost is not proportional to the amount of active parameters.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5911d794-155b-4ab9-a109-eb524c39d3f0",
   "metadata": {},
   "source": [
    "## How to implement MoE in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33773442-6959-44ca-a31c-80b57a840e45",
   "metadata": {},
   "source": [
    "## MoE in SageMaker Jumpstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e35588-a882-4d83-b978-0947debaef0e",
   "metadata": {},
   "source": [
    "## Conclusion / Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3449d1f2-1575-4538-b1a0-419d71934fde",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "- [Stanford CS25: V4 I Demystifying Mixtral of Experts by Albert Jiang](https://www.youtube.com/watch?v=RcJ1YXHLv5o)\n",
    "- [Mixtral of Experts, arxiv: 2401.04088](https://arxiv.org/abs/2401.04088)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8560644-0af2-44d3-a534-fbec8f5d35f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
