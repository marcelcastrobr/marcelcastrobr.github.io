{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed92a7e-adf6-4fb3-98c7-171d52178bd5",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /2024/05/19/UnderstandingMistureOfExperts\n",
    "badges: true\n",
    "categories:\n",
    "- sagemaker\n",
    "- transformers\n",
    "- NLP\n",
    "- MoE\n",
    "date: '2024-05-19'\n",
    "description: Details on Mixture of Experts and how to run it.\n",
    "output-file: 2024-05-19-UnderstandingMistureOfExperts.html\n",
    "title: Understanding Mixture of Experts\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2e63b-a77d-4aa3-9c9a-a71c99bf57c8",
   "metadata": {},
   "source": [
    "# Understanding Misture of Experts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7ecc2-8fca-4e70-8be1-b20dc36a0f08",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "Basic knowledge of Python.\n",
    "\n",
    "Access to Amazon SageMaker Jumpstart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668795f5-1743-44df-8a25-a62393790332",
   "metadata": {},
   "source": [
    "## What is Mixture of Experts\n",
    "\n",
    "Mixture of Experts (MoE) idea dates back to 2010, where it has been explored for example in SVMs and Gaussian Process (ref. [Learning Factored Representations in a Deep Mixture of Experts\n",
    "](https://arxiv.org/abs/1312.4314)). Lately is has incorporated in LSTM with the introduction of sparsity (i.e. to allow running only parts of the whole neural network) (ref. [Switch Transformers](https://arxiv.org/abs/2101.03961)).\n",
    "\n",
    "The general idea of MoE is to replicate certain model components many times while routing each input only to a small subset of those replicas (a.k.a. experts).\n",
    "MoEs achieve faster inference for same model quality at the expense of significatly higher memory cost as all replicated components (a.k.a. parameter) need to be loaded in memory.\n",
    "\n",
    "\n",
    "Mixture of Experts (MoE) consists of the main two elements:\n",
    "\n",
    "#### Sparse MoE layer\n",
    "Instead of using dense feed-forward network (FFN), MoE makes use of sparse MoE layers known as \"experts\". As show in picture below, each expert is a neural network.\n",
    "\n",
    "\n",
    "#### Router or gate network\n",
    "\n",
    "In an MoE, the router determines which tokens are sent to which experts. The router is complsed by the learned parameters and its pre-trained at the same time as the rest of the network.\n",
    "\n",
    "![moe](./images/moe.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### MoE Benefits:\n",
    "The conditional computation on MoE where parts of the network are active on a per-token basis, allow us to scale the size of the model without increasing the computation.\n",
    "\n",
    "### MoE Challenges:\n",
    "\n",
    "As described by [Mixture of Experts Explained by HuggingFace](https://huggingface.co/blog/moe), MoE comes with some challenges: \n",
    "\n",
    "- **Training**: MoEs enable significantly more compute-efficient pretraining, but theyâ€™ve historically struggled to generalize during fine-tuning, leading to overfitting.\n",
    "\n",
    "- **High memory requirement at inference**: Although a MoE might have many parameters, only some of them are used during inference. This leads to fast inference compared to a dense model with same number of parameters. But, ALL parameters need to be loaded in RAM, so memory requirements are high. This is a disadvantage for MoE in edge devices as memory size is restricted. \n",
    "\n",
    "### MoE - Future Works:\n",
    "\n",
    " [QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models](https://arxiv.org/pdf/2310.16795)) proposed a new compression framework called QMoE which uses *quantization* as a way to compress trillion-parameters MoEs to less than 1 bit per parameter. Basically quantization converst the parameters (a.k.a model weights) to lower numerical precision (e.g. going from 16bits - half precision to 4 bits per weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd852b54-fd3b-496b-8503-9840350c6dd9",
   "metadata": {},
   "source": [
    "## Misture of Experts Myths\n",
    "\n",
    "### Myth 1: There are 8 experts in Mixtral 8x7B\n",
    "\n",
    "Every transformer layer has 8 experts and they are permuted in each layer.\n",
    "Instead of 8 experts what we have is a 256 independent experts in total accross the layers (32 x 8). \n",
    "\n",
    "###  Myth 2: There are 56B parameters in Mixtral 8x7B\n",
    "\n",
    "In reallity there are not 56B (8x7B) but 46.7B as the gating and attention layers are shared among the experts. Thus each token will see 12.9B active parameters instead of 14B parameters.\n",
    "\n",
    "### Myth 3: Cost and amount of active parameters are proportional\n",
    "\n",
    "Mixtral 8x7B has fewer active parameters than Llama2 13B.\n",
    "But by having expert routing in MoE you have a higher communication cost as you need to send tokens to different experts. Thus the cost and amount of active parameters are NOT proportiona in MoE.\n",
    "\n",
    "Note that in MoE you can not program which token you send to which expert. Thus while gaining on performance/cost, the absolute cost is not proportional to the amount of active parameters.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5911d794-155b-4ab9-a109-eb524c39d3f0",
   "metadata": {},
   "source": [
    "## How to implement MoE in PyTorch\n",
    "\n",
    "ToBeDefine\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33773442-6959-44ca-a31c-80b57a840e45",
   "metadata": {},
   "source": [
    "## Deploy Mixtral 8x7B Instruct MoE using SageMaker Jumpstart\n",
    "\n",
    "This notebook is inspired by Amazon SageMaker Jumpstart notebooks, which uses SageMaker Python SDK to deploy [Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) text generation model.\n",
    "\n",
    "We use the instance ml.g5.48xlarge which contains 8 x NVIDIA A10g  with a total 192 GB memory. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b2b89fd-5378-43ad-9a96-132c339fc3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "2.221.0\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__) # 2.214.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20931d96-556d-4486-9c81-151132dc2a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade sagemaker #2.221.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c46355-2f40-4d03-8863-fd58a075cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e8a2f20-ed71-455c-9cd8-fb87730ae42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"huggingface-llm-mixtral-8x7b-instruct\" #\"huggingface-llm-mistral-7b-instruct\" #\n",
    "accept_eula = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0159171-3def-4f0d-89cc-9ba2cf343420",
   "metadata": {},
   "source": [
    "### Deploying the model\n",
    "\n",
    "We deploy Mixtral 8x7B using Amazon SageMaker Jumpstart.\n",
    "[Amazon SageMaker Jumpstart](https://aws.amazon.com/sagemaker/jumpstart/) is a machine learning (ML) Hub with foundation models (FM), build-in algorithms and pre-build ML solutions that you can deploy with just a few clicks. \n",
    "\n",
    "For further information ref. AWS ML Blog [Mixtral-8x7B is now available in Amazon SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/mixtral-8x7b-is-now-available-in-amazon-sagemaker-jumpstart/).\n",
    "\n",
    "For a complete list of all pre-trained model in Amazon SageMaker Jumpstart please check: https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html \n",
    "\n",
    "We make use of Amazon SageMaker Jumpstart [JumpStartModel class](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.jumpstart.model.JumpStartModel) to deploy the model. You can also use Amazon SageMaker Jumpstart to fine-tune a foundation model by using [JumpStartEstimator class](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-use-python-sdk.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f7361e3-5092-4892-9cda-47d48dcef9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'huggingface-llm-mixtral-8x7b-instruct' with wildcard version identifier '*'. You can pin to version '1.4.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "import json\n",
    "number_of_gpu = 4\n",
    "\n",
    "config = {\n",
    "  'HF_API_TOKEN': \"XX\",\n",
    "    'SM_NUM_GPUS': json.dumps(number_of_gpu),\n",
    "    'HF_MODEL_QUANTIZE': \"bitsandbytes-nf4\",\n",
    "}\n",
    "model = JumpStartModel(model_id=model_id, env=config)\n",
    "\n",
    "# By default sagemaker expects an ml.p4d.24xlarge instance (NVIDIA A100 - 8 GPUs and 320 GB memory).\n",
    "# Due to quota restriction decided to use ml.g5.12xlarge (NVIDIA A10g  - 4 GPUs and 96 GB memory)\n",
    "predictor = model.deploy(\n",
    "    accept_eula=accept_eula,\n",
    "    instance_type= 'ml.g5.12xlarge',\n",
    "    container_startup_health_check_timeout= 2000) # 10 minutes to be able to load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb66be-0405-4284-9dda-db5e624c336e",
   "metadata": {},
   "source": [
    "### Invoke the endpoint\n",
    "With the endpoint deployed we can now run inference. \n",
    "We will use the `predict` method from the `predictor` to run inference on our endpoint. \n",
    "We can call the model with different parameters to impact the text generation. For the list of parameters available for the model check [Philschmid blog](https://www.philschmid.de/sagemaker-llama-llm#5-run-inference-and-chat-with-the-model).\n",
    "\n",
    "\n",
    "The `mistralai/Mixtral-8x7B-Instruct-v0.1` is a conversational chat model meaning we can chat with it using the following prompt:\n",
    "  \n",
    "```\n",
    "<s> [INST] User Instruction 1 [/INST] Model answer 1</s> [INST] User instruction 2 [/INST]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e591a829-58a3-4c84-b9d2-5b4bd5822a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '<s> [INST] Simply put, the theory of relativity states that [/INST] The theory of relativity, developed by Albert Einstein, is actually composed of two parts: the special theory of relativity and the general theory of relativity.\\n\\nThe special theory of relativity, proposed in 1905, states that the laws of physics are the same for all observers moving at'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt= f'<s> [INST] Simply put, the theory of relativity states that [/INST]'\n",
    "\n",
    "payload = {\n",
    "    'inputs': prompt,\n",
    "    'parameters': {\n",
    "        'max_new_tokens':64,\n",
    "        'top_p':0.9, \n",
    "        'temperature': 0.6,\n",
    "        'stop': ['</s>']\n",
    "    }\n",
    "}\n",
    "predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb8503-c89d-42a8-9300-d0f4b5cc8de7",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "\n",
    "After you are done running the notebook, make sure to delete all the resources that you created in the process to make sure your billing is stopped.\n",
    "Use the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26b45134-fde3-4c5d-954a-49078d05b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e35588-a882-4d83-b978-0947debaef0e",
   "metadata": {},
   "source": [
    "## Conclusion / Remarks \n",
    "ToBeDefine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3449d1f2-1575-4538-b1a0-419d71934fde",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "- [Stanford CS25: V4 I Demystifying Mixtral of Experts by Albert Jiang](https://www.youtube.com/watch?v=RcJ1YXHLv5o)\n",
    "- [Mixtral of Experts, arxiv: 2401.04088](https://arxiv.org/abs/2401.04088)\n",
    "- [Deploying Mistral 7B with NeuronX on Inf2 through SageMaker LMI container and streaming outputs](https://github.com/ghimiresunil/LLM-PowerHouse-A-Curated-Guide-for-Large-Language-Models-with-Custom-Training-and-Inferencing/blob/main/example_codebase/Inf2%20LLM%20SM%20Deployment/mistral7b_inf2.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8560644-0af2-44d3-a534-fbec8f5d35f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
